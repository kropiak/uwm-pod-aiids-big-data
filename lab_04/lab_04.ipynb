{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27e4442b-6e13-4a4b-9a4f-0d3676bd78ad",
   "metadata": {},
   "source": [
    "# lab 4 - Wykorzystanie biblioteki Dask w zadaniach Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2742d07-71d0-4335-839e-eb3faa714303",
   "metadata": {},
   "source": [
    "## 1. Eksperymetny bez wykorzystania biblioteki Dask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a754d23d-7b6f-4c2b-bb8b-1990a3874b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn tqdm xgboost ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f36a3a-1993-48d2-92fb-4a6af4768b77",
   "metadata": {},
   "source": [
    "**Przykład 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0428bd3b-c4a7-48ba-8cbb-952ea4549e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_rcv1, fetch_covtype\n",
    "import numpy as np\n",
    "\n",
    "# pobranie zbioru RCV1 i zapisanie w postaci binarnej za pomocą ZARR\n",
    "# dokumentacja sklearn z opcjami pobrania zbioru: https://scikit-learn.org/1.5/modules/generated/sklearn.datasets.fetch_rcv1.html#sklearn.datasets.fetch_rcv1\n",
    "# rcv1 = fetch_rcv1()\n",
    "# dane w oryginale są zwracane jako tablice rzadkie (sparse) i trzeba je zamienić\n",
    "# na tablicę gęstą\n",
    "\n",
    "# UWAGA: ta operacja spowoduje potrzebę zaalokowania około 283 GB pamięci RAM, co zapewne się nie uda\n",
    "# więc tutaj trzeba będzie użyć klastra do realizacji tego zadania\n",
    "# X, y = rcv1.data.toarray(), rcv1.target.toarray()\n",
    "\n",
    "# mniejszy zbiór do przeprowadzenia przykładu\n",
    "cov = fetch_covtype()\n",
    "X, y = cov.data, cov.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "915bafe2-7e70-47a6-8f15-351329310629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "\n",
    "data = [X, da.atleast_2d(y).T]\n",
    "ddf = da.concatenate(data, axis=1).to_dask_dataframe(columns=cov.feature_names + cov.target_names, index=None).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cab1f5ce-3d06-446f-9138-e48ca8bf1785",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = ddf.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "966dff42-6531-4b0f-9ffc-cc48d4394bd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Elevation</th>\n",
       "      <th>Aspect</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Horizontal_Distance_To_Hydrology</th>\n",
       "      <th>Vertical_Distance_To_Hydrology</th>\n",
       "      <th>Horizontal_Distance_To_Roadways</th>\n",
       "      <th>Hillshade_9am</th>\n",
       "      <th>Hillshade_Noon</th>\n",
       "      <th>Hillshade_3pm</th>\n",
       "      <th>...</th>\n",
       "      <th>Soil_Type_31</th>\n",
       "      <th>Soil_Type_32</th>\n",
       "      <th>Soil_Type_33</th>\n",
       "      <th>Soil_Type_34</th>\n",
       "      <th>Soil_Type_35</th>\n",
       "      <th>Soil_Type_36</th>\n",
       "      <th>Soil_Type_37</th>\n",
       "      <th>Soil_Type_38</th>\n",
       "      <th>Soil_Type_39</th>\n",
       "      <th>Cover_Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2596.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>258.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>510.0</td>\n",
       "      <td>221.0</td>\n",
       "      <td>232.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2590.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>212.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>390.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2804.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>268.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>3180.0</td>\n",
       "      <td>234.0</td>\n",
       "      <td>238.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2785.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>3090.0</td>\n",
       "      <td>238.0</td>\n",
       "      <td>238.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2595.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>234.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  Elevation  Aspect  Slope  Horizontal_Distance_To_Hydrology  \\\n",
       "0      0     2596.0    51.0    3.0                             258.0   \n",
       "1      1     2590.0    56.0    2.0                             212.0   \n",
       "2      2     2804.0   139.0    9.0                             268.0   \n",
       "3      3     2785.0   155.0   18.0                             242.0   \n",
       "4      4     2595.0    45.0    2.0                             153.0   \n",
       "\n",
       "   Vertical_Distance_To_Hydrology  Horizontal_Distance_To_Roadways  \\\n",
       "0                             0.0                            510.0   \n",
       "1                            -6.0                            390.0   \n",
       "2                            65.0                           3180.0   \n",
       "3                           118.0                           3090.0   \n",
       "4                            -1.0                            391.0   \n",
       "\n",
       "   Hillshade_9am  Hillshade_Noon  Hillshade_3pm  ...  Soil_Type_31  \\\n",
       "0          221.0           232.0          148.0  ...           0.0   \n",
       "1          220.0           235.0          151.0  ...           0.0   \n",
       "2          234.0           238.0          135.0  ...           0.0   \n",
       "3          238.0           238.0          122.0  ...           0.0   \n",
       "4          220.0           234.0          150.0  ...           0.0   \n",
       "\n",
       "   Soil_Type_32  Soil_Type_33  Soil_Type_34  Soil_Type_35  Soil_Type_36  \\\n",
       "0           0.0           0.0           0.0           0.0           0.0   \n",
       "1           0.0           0.0           0.0           0.0           0.0   \n",
       "2           0.0           0.0           0.0           0.0           0.0   \n",
       "3           0.0           0.0           0.0           0.0           0.0   \n",
       "4           0.0           0.0           0.0           0.0           0.0   \n",
       "\n",
       "   Soil_Type_37  Soil_Type_38  Soil_Type_39  Cover_Type  \n",
       "0           0.0           0.0           0.0         5.0  \n",
       "1           0.0           0.0           0.0         5.0  \n",
       "2           0.0           0.0           0.0         2.0  \n",
       "3           0.0           0.0           0.0         2.0  \n",
       "4           0.0           0.0           0.0         5.0  \n",
       "\n",
       "[5 rows x 56 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0aa20b31-147d-4d5a-a624-0ccabd020258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Elevation</th>\n",
       "      <th>Aspect</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Horizontal_Distance_To_Hydrology</th>\n",
       "      <th>Vertical_Distance_To_Hydrology</th>\n",
       "      <th>Horizontal_Distance_To_Roadways</th>\n",
       "      <th>Hillshade_9am</th>\n",
       "      <th>Hillshade_Noon</th>\n",
       "      <th>Hillshade_3pm</th>\n",
       "      <th>...</th>\n",
       "      <th>Soil_Type_31</th>\n",
       "      <th>Soil_Type_32</th>\n",
       "      <th>Soil_Type_33</th>\n",
       "      <th>Soil_Type_34</th>\n",
       "      <th>Soil_Type_35</th>\n",
       "      <th>Soil_Type_36</th>\n",
       "      <th>Soil_Type_37</th>\n",
       "      <th>Soil_Type_38</th>\n",
       "      <th>Soil_Type_39</th>\n",
       "      <th>Cover_Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>290505.500000</td>\n",
       "      <td>2959.365301</td>\n",
       "      <td>155.656807</td>\n",
       "      <td>14.103704</td>\n",
       "      <td>269.428217</td>\n",
       "      <td>46.418855</td>\n",
       "      <td>2350.146611</td>\n",
       "      <td>212.146049</td>\n",
       "      <td>223.318716</td>\n",
       "      <td>142.528263</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090392</td>\n",
       "      <td>0.077716</td>\n",
       "      <td>0.002773</td>\n",
       "      <td>0.003255</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.000513</td>\n",
       "      <td>0.026803</td>\n",
       "      <td>0.023762</td>\n",
       "      <td>0.015060</td>\n",
       "      <td>2.051471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>167723.861639</td>\n",
       "      <td>279.984734</td>\n",
       "      <td>111.913721</td>\n",
       "      <td>7.488242</td>\n",
       "      <td>212.549356</td>\n",
       "      <td>58.295232</td>\n",
       "      <td>1559.254870</td>\n",
       "      <td>26.769889</td>\n",
       "      <td>19.768697</td>\n",
       "      <td>38.274529</td>\n",
       "      <td>...</td>\n",
       "      <td>0.286743</td>\n",
       "      <td>0.267725</td>\n",
       "      <td>0.052584</td>\n",
       "      <td>0.056957</td>\n",
       "      <td>0.014310</td>\n",
       "      <td>0.022641</td>\n",
       "      <td>0.161508</td>\n",
       "      <td>0.152307</td>\n",
       "      <td>0.121791</td>\n",
       "      <td>1.396504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1859.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-173.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>145252.750000</td>\n",
       "      <td>2809.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1106.000000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>213.000000</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>290505.500000</td>\n",
       "      <td>2996.000000</td>\n",
       "      <td>127.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>218.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>1997.000000</td>\n",
       "      <td>218.000000</td>\n",
       "      <td>226.000000</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>435758.250000</td>\n",
       "      <td>3163.000000</td>\n",
       "      <td>260.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>384.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>3328.000000</td>\n",
       "      <td>231.000000</td>\n",
       "      <td>237.000000</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>581011.000000</td>\n",
       "      <td>3858.000000</td>\n",
       "      <td>360.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>1397.000000</td>\n",
       "      <td>601.000000</td>\n",
       "      <td>7117.000000</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               index      Elevation         Aspect          Slope  \\\n",
       "count  581012.000000  581012.000000  581012.000000  581012.000000   \n",
       "mean   290505.500000    2959.365301     155.656807      14.103704   \n",
       "std    167723.861639     279.984734     111.913721       7.488242   \n",
       "min         0.000000    1859.000000       0.000000       0.000000   \n",
       "25%    145252.750000    2809.000000      58.000000       9.000000   \n",
       "50%    290505.500000    2996.000000     127.000000      13.000000   \n",
       "75%    435758.250000    3163.000000     260.000000      18.000000   \n",
       "max    581011.000000    3858.000000     360.000000      66.000000   \n",
       "\n",
       "       Horizontal_Distance_To_Hydrology  Vertical_Distance_To_Hydrology  \\\n",
       "count                     581012.000000                   581012.000000   \n",
       "mean                         269.428217                       46.418855   \n",
       "std                          212.549356                       58.295232   \n",
       "min                            0.000000                     -173.000000   \n",
       "25%                          108.000000                        7.000000   \n",
       "50%                          218.000000                       30.000000   \n",
       "75%                          384.000000                       69.000000   \n",
       "max                         1397.000000                      601.000000   \n",
       "\n",
       "       Horizontal_Distance_To_Roadways  Hillshade_9am  Hillshade_Noon  \\\n",
       "count                    581012.000000  581012.000000   581012.000000   \n",
       "mean                       2350.146611     212.146049      223.318716   \n",
       "std                        1559.254870      26.769889       19.768697   \n",
       "min                           0.000000       0.000000        0.000000   \n",
       "25%                        1106.000000     198.000000      213.000000   \n",
       "50%                        1997.000000     218.000000      226.000000   \n",
       "75%                        3328.000000     231.000000      237.000000   \n",
       "max                        7117.000000     254.000000      254.000000   \n",
       "\n",
       "       Hillshade_3pm  ...   Soil_Type_31   Soil_Type_32   Soil_Type_33  \\\n",
       "count  581012.000000  ...  581012.000000  581012.000000  581012.000000   \n",
       "mean      142.528263  ...       0.090392       0.077716       0.002773   \n",
       "std        38.274529  ...       0.286743       0.267725       0.052584   \n",
       "min         0.000000  ...       0.000000       0.000000       0.000000   \n",
       "25%       119.000000  ...       0.000000       0.000000       0.000000   \n",
       "50%       143.000000  ...       0.000000       0.000000       0.000000   \n",
       "75%       168.000000  ...       0.000000       0.000000       0.000000   \n",
       "max       254.000000  ...       1.000000       1.000000       1.000000   \n",
       "\n",
       "        Soil_Type_34   Soil_Type_35   Soil_Type_36   Soil_Type_37  \\\n",
       "count  581012.000000  581012.000000  581012.000000  581012.000000   \n",
       "mean        0.003255       0.000205       0.000513       0.026803   \n",
       "std         0.056957       0.014310       0.022641       0.161508   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       0.000000       0.000000       0.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "        Soil_Type_38   Soil_Type_39     Cover_Type  \n",
       "count  581012.000000  581012.000000  581012.000000  \n",
       "mean        0.023762       0.015060       2.051471  \n",
       "std         0.152307       0.121791       1.396504  \n",
       "min         0.000000       0.000000       1.000000  \n",
       "25%         0.000000       0.000000       1.000000  \n",
       "50%         0.000000       0.000000       2.000000  \n",
       "75%         0.000000       0.000000       2.000000  \n",
       "max         1.000000       1.000000       7.000000  \n",
       "\n",
       "[8 rows x 56 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05c9b614-ec52-46ac-b029-ba5db429aeb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# uwaga na faktyczny typ danych tej ramki!\n",
    "type(ddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44948bce-6559-488a-b69f-3ae13faefada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fastparquet\n",
    "\n",
    "DATADIR = './data/cov/'\n",
    "os.makedirs(DATADIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b709cfb2-9d65-4b86-9b59-6e051cf4a696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to pandas DataFrame (brak podziału na partycje)\n",
    "ddf.to_parquet(os.path.join(DATADIR, 'data.parquet'))\n",
    "\n",
    "# ddf.to_parquet(DATADIR, name_function=lambda x: f\"data-{x}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b39221e3-9c05-4747-94f5-6c4c9b3e16b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jako, że ta wersja biblioteki XGBoost wymaga, aby wartości dla target(y) były indeksowane od 0\n",
    "# należy wykonać enkodowanie dla tego datasetu\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1949a5e2-30c1-449b-8f23-58fdbb3e7a3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b495d44be6844d30b2779b18969948f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:1.42195\n",
      "[1]\tvalidation_0-mlogloss:1.17182\n",
      "[2]\tvalidation_0-mlogloss:1.00835\n",
      "[3]\tvalidation_0-mlogloss:0.89389\n",
      "[4]\tvalidation_0-mlogloss:0.81015\n",
      "[5]\tvalidation_0-mlogloss:0.74716\n",
      "[6]\tvalidation_0-mlogloss:0.69977\n",
      "[7]\tvalidation_0-mlogloss:0.66108\n",
      "[8]\tvalidation_0-mlogloss:0.63163\n",
      "[9]\tvalidation_0-mlogloss:0.60702\n",
      "[10]\tvalidation_0-mlogloss:0.58751\n",
      "[11]\tvalidation_0-mlogloss:0.56978\n",
      "[12]\tvalidation_0-mlogloss:0.55626\n",
      "[13]\tvalidation_0-mlogloss:0.54217\n",
      "[14]\tvalidation_0-mlogloss:0.52944\n",
      "[15]\tvalidation_0-mlogloss:0.52098\n",
      "[16]\tvalidation_0-mlogloss:0.51338\n",
      "[17]\tvalidation_0-mlogloss:0.50204\n",
      "[18]\tvalidation_0-mlogloss:0.49490\n",
      "[19]\tvalidation_0-mlogloss:0.48963\n",
      "[20]\tvalidation_0-mlogloss:0.48454\n",
      "[21]\tvalidation_0-mlogloss:0.48032\n",
      "[22]\tvalidation_0-mlogloss:0.47495\n",
      "[23]\tvalidation_0-mlogloss:0.47029\n",
      "[24]\tvalidation_0-mlogloss:0.46705\n",
      "[25]\tvalidation_0-mlogloss:0.46266\n",
      "[26]\tvalidation_0-mlogloss:0.45989\n",
      "[27]\tvalidation_0-mlogloss:0.45814\n",
      "[28]\tvalidation_0-mlogloss:0.45482\n",
      "[29]\tvalidation_0-mlogloss:0.45229\n",
      "[30]\tvalidation_0-mlogloss:0.45064\n",
      "[31]\tvalidation_0-mlogloss:0.44864\n",
      "[32]\tvalidation_0-mlogloss:0.44466\n",
      "[33]\tvalidation_0-mlogloss:0.44138\n",
      "[34]\tvalidation_0-mlogloss:0.43839\n",
      "[35]\tvalidation_0-mlogloss:0.43649\n",
      "[36]\tvalidation_0-mlogloss:0.43363\n",
      "[37]\tvalidation_0-mlogloss:0.43121\n",
      "[38]\tvalidation_0-mlogloss:0.42826\n",
      "[39]\tvalidation_0-mlogloss:0.42613\n",
      "[40]\tvalidation_0-mlogloss:0.42454\n",
      "[41]\tvalidation_0-mlogloss:0.42332\n",
      "[42]\tvalidation_0-mlogloss:0.42196\n",
      "[43]\tvalidation_0-mlogloss:0.41977\n",
      "[44]\tvalidation_0-mlogloss:0.41682\n",
      "[45]\tvalidation_0-mlogloss:0.41513\n",
      "[46]\tvalidation_0-mlogloss:0.41335\n",
      "[47]\tvalidation_0-mlogloss:0.41225\n",
      "[48]\tvalidation_0-mlogloss:0.41105\n",
      "[49]\tvalidation_0-mlogloss:0.40667\n",
      "[50]\tvalidation_0-mlogloss:0.40295\n",
      "[51]\tvalidation_0-mlogloss:0.39969\n",
      "[52]\tvalidation_0-mlogloss:0.39762\n",
      "[53]\tvalidation_0-mlogloss:0.39670\n",
      "[54]\tvalidation_0-mlogloss:0.39545\n",
      "[55]\tvalidation_0-mlogloss:0.39451\n",
      "[56]\tvalidation_0-mlogloss:0.39283\n",
      "[57]\tvalidation_0-mlogloss:0.39147\n",
      "[58]\tvalidation_0-mlogloss:0.38917\n",
      "[59]\tvalidation_0-mlogloss:0.38698\n",
      "[60]\tvalidation_0-mlogloss:0.38468\n",
      "[61]\tvalidation_0-mlogloss:0.38242\n",
      "[62]\tvalidation_0-mlogloss:0.38030\n",
      "[63]\tvalidation_0-mlogloss:0.37884\n",
      "[64]\tvalidation_0-mlogloss:0.37761\n",
      "[65]\tvalidation_0-mlogloss:0.37688\n",
      "[66]\tvalidation_0-mlogloss:0.37504\n",
      "[67]\tvalidation_0-mlogloss:0.37297\n",
      "[68]\tvalidation_0-mlogloss:0.37163\n",
      "[69]\tvalidation_0-mlogloss:0.37037\n",
      "[70]\tvalidation_0-mlogloss:0.36853\n",
      "[71]\tvalidation_0-mlogloss:0.36612\n",
      "[72]\tvalidation_0-mlogloss:0.36423\n",
      "[73]\tvalidation_0-mlogloss:0.36288\n",
      "[74]\tvalidation_0-mlogloss:0.36097\n",
      "[75]\tvalidation_0-mlogloss:0.35981\n",
      "[76]\tvalidation_0-mlogloss:0.35747\n",
      "[77]\tvalidation_0-mlogloss:0.35586\n",
      "[78]\tvalidation_0-mlogloss:0.35384\n",
      "[79]\tvalidation_0-mlogloss:0.35306\n",
      "[80]\tvalidation_0-mlogloss:0.35210\n",
      "[81]\tvalidation_0-mlogloss:0.35134\n",
      "[82]\tvalidation_0-mlogloss:0.35023\n",
      "[83]\tvalidation_0-mlogloss:0.34940\n",
      "[84]\tvalidation_0-mlogloss:0.34747\n",
      "[85]\tvalidation_0-mlogloss:0.34619\n",
      "[86]\tvalidation_0-mlogloss:0.34389\n",
      "[87]\tvalidation_0-mlogloss:0.34297\n",
      "[88]\tvalidation_0-mlogloss:0.34087\n",
      "[89]\tvalidation_0-mlogloss:0.33795\n",
      "[90]\tvalidation_0-mlogloss:0.33652\n",
      "[91]\tvalidation_0-mlogloss:0.33452\n",
      "[92]\tvalidation_0-mlogloss:0.33294\n",
      "[93]\tvalidation_0-mlogloss:0.33224\n",
      "[94]\tvalidation_0-mlogloss:0.33072\n",
      "[95]\tvalidation_0-mlogloss:0.32990\n",
      "[96]\tvalidation_0-mlogloss:0.32827\n",
      "[97]\tvalidation_0-mlogloss:0.32713\n",
      "[98]\tvalidation_0-mlogloss:0.32611\n",
      "[99]\tvalidation_0-mlogloss:0.32501\n",
      "[0]\tvalidation_0-mlogloss:1.42176\n",
      "[1]\tvalidation_0-mlogloss:1.17143\n",
      "[2]\tvalidation_0-mlogloss:1.00867\n",
      "[3]\tvalidation_0-mlogloss:0.89337\n",
      "[4]\tvalidation_0-mlogloss:0.81032\n",
      "[5]\tvalidation_0-mlogloss:0.74760\n",
      "[6]\tvalidation_0-mlogloss:0.69993\n",
      "[7]\tvalidation_0-mlogloss:0.66248\n",
      "[8]\tvalidation_0-mlogloss:0.63293\n",
      "[9]\tvalidation_0-mlogloss:0.60745\n",
      "[10]\tvalidation_0-mlogloss:0.58742\n",
      "[11]\tvalidation_0-mlogloss:0.56994\n",
      "[12]\tvalidation_0-mlogloss:0.55458\n",
      "[13]\tvalidation_0-mlogloss:0.54131\n",
      "[14]\tvalidation_0-mlogloss:0.53070\n",
      "[15]\tvalidation_0-mlogloss:0.52299\n",
      "[16]\tvalidation_0-mlogloss:0.51322\n",
      "[17]\tvalidation_0-mlogloss:0.50385\n",
      "[18]\tvalidation_0-mlogloss:0.49852\n",
      "[19]\tvalidation_0-mlogloss:0.49282\n",
      "[20]\tvalidation_0-mlogloss:0.48832\n",
      "[21]\tvalidation_0-mlogloss:0.48162\n",
      "[22]\tvalidation_0-mlogloss:0.47751\n",
      "[23]\tvalidation_0-mlogloss:0.47346\n",
      "[24]\tvalidation_0-mlogloss:0.47022\n",
      "[25]\tvalidation_0-mlogloss:0.46638\n",
      "[26]\tvalidation_0-mlogloss:0.46355\n",
      "[27]\tvalidation_0-mlogloss:0.46158\n",
      "[28]\tvalidation_0-mlogloss:0.45622\n",
      "[29]\tvalidation_0-mlogloss:0.45384\n",
      "[30]\tvalidation_0-mlogloss:0.45203\n",
      "[31]\tvalidation_0-mlogloss:0.44892\n",
      "[32]\tvalidation_0-mlogloss:0.44661\n",
      "[33]\tvalidation_0-mlogloss:0.44495\n",
      "[34]\tvalidation_0-mlogloss:0.44189\n",
      "[35]\tvalidation_0-mlogloss:0.43974\n",
      "[36]\tvalidation_0-mlogloss:0.43739\n",
      "[37]\tvalidation_0-mlogloss:0.43600\n",
      "[38]\tvalidation_0-mlogloss:0.43351\n",
      "[39]\tvalidation_0-mlogloss:0.43205\n",
      "[40]\tvalidation_0-mlogloss:0.42995\n",
      "[41]\tvalidation_0-mlogloss:0.42902\n",
      "[42]\tvalidation_0-mlogloss:0.42731\n",
      "[43]\tvalidation_0-mlogloss:0.42499\n",
      "[44]\tvalidation_0-mlogloss:0.42272\n",
      "[45]\tvalidation_0-mlogloss:0.42009\n",
      "[46]\tvalidation_0-mlogloss:0.41684\n",
      "[47]\tvalidation_0-mlogloss:0.41267\n",
      "[48]\tvalidation_0-mlogloss:0.40983\n",
      "[49]\tvalidation_0-mlogloss:0.40750\n",
      "[50]\tvalidation_0-mlogloss:0.40570\n",
      "[51]\tvalidation_0-mlogloss:0.40429\n",
      "[52]\tvalidation_0-mlogloss:0.40375\n",
      "[53]\tvalidation_0-mlogloss:0.40121\n",
      "[54]\tvalidation_0-mlogloss:0.39915\n",
      "[55]\tvalidation_0-mlogloss:0.39631\n",
      "[56]\tvalidation_0-mlogloss:0.39461\n",
      "[57]\tvalidation_0-mlogloss:0.39107\n",
      "[58]\tvalidation_0-mlogloss:0.38803\n",
      "[59]\tvalidation_0-mlogloss:0.38658\n",
      "[60]\tvalidation_0-mlogloss:0.38513\n",
      "[61]\tvalidation_0-mlogloss:0.38264\n",
      "[62]\tvalidation_0-mlogloss:0.38058\n",
      "[63]\tvalidation_0-mlogloss:0.37885\n",
      "[64]\tvalidation_0-mlogloss:0.37747\n",
      "[65]\tvalidation_0-mlogloss:0.37576\n",
      "[66]\tvalidation_0-mlogloss:0.37241\n",
      "[67]\tvalidation_0-mlogloss:0.37037\n",
      "[68]\tvalidation_0-mlogloss:0.36841\n",
      "[69]\tvalidation_0-mlogloss:0.36788\n",
      "[70]\tvalidation_0-mlogloss:0.36720\n",
      "[71]\tvalidation_0-mlogloss:0.36613\n",
      "[72]\tvalidation_0-mlogloss:0.36480\n",
      "[73]\tvalidation_0-mlogloss:0.36367\n",
      "[74]\tvalidation_0-mlogloss:0.36155\n",
      "[75]\tvalidation_0-mlogloss:0.36028\n",
      "[76]\tvalidation_0-mlogloss:0.35801\n",
      "[77]\tvalidation_0-mlogloss:0.35635\n",
      "[78]\tvalidation_0-mlogloss:0.35421\n",
      "[79]\tvalidation_0-mlogloss:0.35287\n",
      "[80]\tvalidation_0-mlogloss:0.35155\n",
      "[81]\tvalidation_0-mlogloss:0.35012\n",
      "[82]\tvalidation_0-mlogloss:0.34880\n",
      "[83]\tvalidation_0-mlogloss:0.34770\n",
      "[84]\tvalidation_0-mlogloss:0.34679\n",
      "[85]\tvalidation_0-mlogloss:0.34624\n",
      "[86]\tvalidation_0-mlogloss:0.34502\n",
      "[87]\tvalidation_0-mlogloss:0.34408\n",
      "[88]\tvalidation_0-mlogloss:0.34237\n",
      "[89]\tvalidation_0-mlogloss:0.34066\n",
      "[90]\tvalidation_0-mlogloss:0.33853\n",
      "[91]\tvalidation_0-mlogloss:0.33640\n",
      "[92]\tvalidation_0-mlogloss:0.33538\n",
      "[93]\tvalidation_0-mlogloss:0.33426\n",
      "[94]\tvalidation_0-mlogloss:0.33353\n",
      "[95]\tvalidation_0-mlogloss:0.33231\n",
      "[96]\tvalidation_0-mlogloss:0.33103\n",
      "[97]\tvalidation_0-mlogloss:0.32995\n",
      "[98]\tvalidation_0-mlogloss:0.32918\n",
      "[99]\tvalidation_0-mlogloss:0.32865\n",
      "[0]\tvalidation_0-mlogloss:1.42116\n",
      "[1]\tvalidation_0-mlogloss:1.17126\n",
      "[2]\tvalidation_0-mlogloss:1.00631\n",
      "[3]\tvalidation_0-mlogloss:0.89097\n",
      "[4]\tvalidation_0-mlogloss:0.80859\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import clone\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "# X, y = load_breast_cancer(return_X_y=True)\n",
    "n_splits = 5\n",
    "\n",
    "def fit_and_score(estimator, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Fit the estimator on the train set and score it on both sets\"\"\"\n",
    "    estimator.fit(X_train, y_train, eval_set=[(X_test, y_test)])\n",
    "\n",
    "    train_score = estimator.score(X_train, y_train)\n",
    "    test_score = estimator.score(X_test, y_test)\n",
    "\n",
    "    return estimator, train_score, test_score\n",
    "\n",
    "\n",
    "cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=94)\n",
    "\n",
    "\n",
    "# ta implementacja algorytmu XGBoost wykorzystuje joblib i domyślnie jego praca jest zrównoleglana\n",
    "# wydajność użycia klasycznego podejścia vs. Dask na klastrze lokalnym prawdopodobnie przyniesie gorsze\n",
    "# rezultaty dla tego drugiego rozwiązania, ale wymaga to sprawdzenia\n",
    "\n",
    "# jednak jeżeli weźmiemy pod uwagę ograniczenia pamięci, które możemy napotkać pracując bez wykorzystania\n",
    "# daska, może okazać się, że podejście klasyczne nie będzie możliwe do uruchomienia jeżeli nie\n",
    "# dysponujemy wystarczającą ilością zasobów sprzętowych\n",
    "\n",
    "clf = xgb.XGBClassifier(tree_method=\"hist\", early_stopping_rounds=3)\n",
    "\n",
    "results = {}\n",
    "\n",
    "for train, test in tqdm(cv.split(X, y), desc=\"Training...\", total=n_splits):\n",
    "    X_train = X[train]\n",
    "    X_test = X[test]\n",
    "    y_train = y[train]\n",
    "    y_test = y[test]\n",
    "    est, train_score, test_score = fit_and_score(\n",
    "        clone(clf), X_train, X_test, y_train, y_test\n",
    "    )\n",
    "    results[est] = (train_score, test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c879ba6c-80ee-4eba-926a-eac362ca37c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold_n, fold in enumerate(results.items(), start=1):\n",
    "    print(f\"Fold {fold_n}: train_score: {fold[1][0]}, test_score: {fold[1][1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b1dbce-c1bd-4767-a623-7b3a9c74162c",
   "metadata": {},
   "source": [
    "## 2. Eksperyment z wykorzystaniem Dask."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1df3eb6-ccb2-4b81-b149-49e5b84fef06",
   "metadata": {},
   "source": [
    "### 2.1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dde4c2-75d0-4ead-babc-2de134d517d0",
   "metadata": {},
   "source": [
    "**Przykład 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0426ac65-9c39-4b07-9e35-45fa5e7b91d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "from dask_ml.model_selection import KFold\n",
    "import xgboost as xgb\n",
    "from dask.distributed import LocalCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4b34df-d1bd-4f1a-937d-3787b3a275b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# konwersja numpy array na Dask array\n",
    "X = da.from_array(X)\n",
    "y = da.from_array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd5dd4f-4162-454a-b462-50b206412b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# szczegóły definiowania parametrów: https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters\n",
    "# tu klasyfikacja wieloklasowa\n",
    "\n",
    "params = {'objective': 'multi:softmax',\n",
    "          'max_depth': 4, 'eta': 0.01, 'subsample': 0.5,\n",
    "          'min_child_weight': 0.5,\n",
    "          'num_class': 7}\n",
    "\n",
    "\n",
    "n_splits = 5\n",
    "cv = KFold(n_splits=n_splits, shuffle=True, random_state=94)\n",
    "\n",
    "predictions = {}\n",
    "\n",
    "with LocalCluster(n_workers=4) as cluster:\n",
    "    display(cluster)\n",
    "    with cluster.get_client() as client:\n",
    "\n",
    "        for i, (train, test) in enumerate(cv.split(X, y)):\n",
    "            \n",
    "            X_train = X[train, :-1]\n",
    "            X_test = X[test, :-1]\n",
    "            y_train = y[train]\n",
    "            y_test = y[test]\n",
    "            \n",
    "            d_train = xgb.dask.DaskDMatrix(client, X_train, y_train, enable_categorical=True)\n",
    "            model = xgb.dask.train(client, params=params, dtrain=d_train)\n",
    "            predictions[f'fold_{i}'] = xgb.dask.predict(client, model, X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895dd87a-eab7-4176-ab5c-ada0fc2c14e9",
   "metadata": {},
   "source": [
    "### 2.2 Rozbicie całego procesu na niezależne zadania"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64aff09-b212-4310-adbc-fca024b67722",
   "metadata": {},
   "source": [
    "W pierwszej fazie zazwyczaj zajęlibyśmy się procesem ekstrakcji, oczyszczania i wstępnego przetwarzania danych, jednak póki co ten etap zostanie tutaj pominięty.\n",
    "\n",
    "W dokumentacji biblioteki Dask znajdziemy cały dział poruszający temat Machine Learning z wykorzystaniem tej biblioteki na różnych etapach tego procesu.\n",
    "\n",
    "> Dokumentacja Dask ML: https://ml.dask.org/\n",
    "\n",
    "Są tam również przykłady (chociaż dość ubogie i czasem nie działające z najnowszymi wersjami bibliotek zależnych), które obrazują w jakich przypadkach można skorzystać z możliwości Dask w kontekście ML. Należy dość dokładnie przeczytać uwagi i wskazówki, które się tam znajdują gdyż nie wszystkie elementy (np. znane biblioteki do tworzenia modeli ML) współpracują z Dask w tzw. trybie \"out of the box\" i trzeba wykorzystywać wrappery lub specjalnie przygotowane integracje, np. bibliotekę Skorch, która pozwala korzystać z Pytorch w sposób bardziej kompatybilny ze scikit-learn a co za tym idzie również z Dask, gdyż tutaj położono największy nacisk na integrację."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c627145-d240-4499-927e-77659d424698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. ładowanie danych\n",
    "# w zależności od potrzeb dane mogą być przekazane do etapu treningowego w różnym formacie: dask dataframe, dask array lub inny.\n",
    "# dodatkowo dzięki wielu formatom przechowywania danych, szczególnie w kontekście big data, gdzie właściwy dobór bibliotek zależy od\n",
    "# dostępnej infrastruktury.\n",
    "# Dane mogą również być wczytywane i następnie przetwarzane w paczkach (ang. batch), co dodatkowo może narzucić pewne ograniczenia co\n",
    "# do miejsca uruchomienia procesu ładowania danych (może tylko scheduler, a może zdalnie na klastrze) lub konieczność rozproszenia\n",
    "# danych po całym klastrze.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f259a1-da1b-436d-a347-7ef50c607924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Podział danych na potrzeby etapu trenowania modelu.\n",
    "# Jeżeli docelowo dysponujemy dużym zbiorem danych, nie oznacza to wcale, że tak jak klasycznie zazwyczaj się to odbywa,\n",
    "# dzielimy cały zbiór na część treningową oraz testową (np. w proporcjach 80/20, 70/30 czy innych) i uruchamiamy na nich\n",
    "# trening wybranego modelu. Lepszym pomysłem jest dobranie odpowiedniej, reprezentatywnej próbki danych, na których wykonamy\n",
    "# wstępny trening. W zależności od różnorodności zbioru może się okazać, że wyniki będzie wystarczająco dobry. Pamiętajmy również, że\n",
    "# finalna wielkość modelu i ilość zasobów potrzebnych, żeby go przechowywać w stanie dostępnym dla etapu inferencji może być kosztowne\n",
    "# i również często wymaga pewnej optymalizacji. A skoro w danym przypadku mniejszy model jest porównywalnie dobry z modelem większym,\n",
    "# mniejszy wygrywa. Również w kontekście szybkości inferencji w fazie produkcyjnej.\n",
    "\n",
    "\n",
    "# zobacz przykład podziału danych w punkcie 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46244031-188b-498e-b4b6-b9549ab567bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Trening modelu.\n",
    "# Ta faza zazwyczaj zajmuje dużo czasu, w kontekście samego treningu, ale również w kontekście iteracyjnej natury tego etapu.\n",
    "# Szukamy optymalnych parametrów modelu (tu mogą pomóc dodatkowe narzędzia i techniki takie jak Optune, ML Flow, grid search),\n",
    "# eksperymentujemy z doborem danych (tu często poprzedzamy to fazą feature engineering).\n",
    "\n",
    "# trening modelu na dużej ilości danych można wykonać na kilka sposobów.\n",
    "# 1. Użycie modelu ensemble, który trenuje większą ilość mniejszych modeli na fragmentach danych.\n",
    "# Ważne jest, aby podział danych odbył się zgodnie z rozkładem w zbiorze oryginalnym, w przeciwnym wypadku część\n",
    "# modeli może dość mocno wpływać na ogólny wynik całego modelu.\n",
    "# zobacz: https://ml.dask.org/modules/generated/dask_ml.ensemble.BlockwiseVotingClassifier.html#dask_ml.ensemble.BlockwiseVotingClassifier\n",
    "\n",
    "# 2. Wykorzystanie jednej z klas biblioteki Dask, która pozwala wykorzystać modele z biblioteki scikit-learn, które wspierają\n",
    "# operację partial_fit, która pozwala na trenowanie modelu na zbiorze danych uczących, który jest podzielony na części (nie \n",
    "# mylić z paczką, ang. batch, która jest dzielona ze zbioru treningowego) i dzięki temu można tu przekazać np. Dask Array jako dane wejściowe.\n",
    "# zobacz: https://ml.dask.org/modules/generated/dask_ml.wrappers.Incremental.html#dask_ml.wrappers.Incremental\n",
    "# oraz https://scikit-learn.org/0.15/modules/scaling_strategies.html\n",
    "\n",
    "# przykłady\n",
    "\n",
    "# przykład z ofocjalnej dokumentacji: https://examples.dask.org/machine-learning/incremental.html\n",
    "# inne przykłady\n",
    "# https://skorch.readthedocs.io/en/stable/user/parallelism.html\n",
    "# https://github.com/skorch-dev/skorch/blob/master/notebooks/MNIST.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de047e81-b825-4e8e-bbb5-c1a230677b06",
   "metadata": {},
   "source": [
    "**Przykład 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366171fe-e28f-40ea-80c9-824b49a9b2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "# pamiętaj o zamykaniu klientów lub używaniu już wcześniej stworzonego\n",
    "client = Client(n_workers=4, threads_per_worker=2, memory_limit=\"4GB\")\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b456ed3-3444-450b-acfc-ce4b17da7974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# przykład z użyciem incremental learning z dokumentacji dask\n",
    "# z lekką modyfikacją\n",
    "\n",
    "import dask\n",
    "import dask.array as da\n",
    "from dask_ml.datasets import make_classification\n",
    "from dask_ml.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from dask_ml.wrappers import Incremental\n",
    "\n",
    "\n",
    "# dostosuj wielkość zbioru oraz ilość/wielkość chunka\n",
    "# wielkość chunka niedobrana prawidłowo do pamięci na workerze może\n",
    "# skutecznie zakończyć przeliczanie całego grafu\n",
    "# przy parametrach poniżej potrzeba około 38GB pamięci RAM, ale przy dobrze\n",
    "# dobranych chunkach obliczymy to na dużo mniejszej ilości zasobów\n",
    "# wykonanie poniższego kodu na mojej maszynie z zadanymi parametrami klastra lokalnego\n",
    "# zajęło kilkanaście minut\n",
    "n, d = 10000000, 500\n",
    "X, y = make_classification(n_samples=n, n_features=d,\n",
    "                           chunks=n // 64, flip_y=0.2)\n",
    "display(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "display(X_train)\n",
    "\n",
    "# jeżeli dysponujemy wystarczająco dużą ilością pamięci RAM rozproszoną po workerach\n",
    "# to możemy przechować dane właśnie tam w celu przyspieszenia części obliczeń\n",
    "# X_train, X_test, y_train, y_test = dask.persist(X_train, X_test, y_train, y_test)\n",
    "\n",
    "\n",
    "classes = da.unique(y_train).compute()\n",
    "# classes\n",
    "\n",
    "est = SGDClassifier(loss='log_loss', penalty='l2', tol=1e-3)\n",
    "inc = Incremental(est, scoring='accuracy')\n",
    "\n",
    "inc.fit(X_train, y_train, classes=classes)\n",
    "inc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b220312-b804-481d-8f1e-71b9204f7d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Serializacja modelu i jego wczytywanie.\n",
    "# Serializacja modelu jest konieczna ze względu na chęć przechowania go w formie bardziej trwałej niż w pamięci operacyjnej, ale\n",
    "# również ze względu na niski koszt jego wczytania wględem konieczności ponownego jego trenowania, to oczywiste.\n",
    "# Często również w cyklu życia modeli ML następuje ich aktualizacja oraz archiwizacja modeli aktualnie nie używanych.\n",
    "\n",
    "\n",
    "def save_model(model):\n",
    "    pass\n",
    "\n",
    "def load_model(path):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458c52a2-963b-4c8d-be0c-7502cf4f2593",
   "metadata": {},
   "source": [
    "**Przykład 4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8304e3ae-a5af-45bc-abac-be8ccf06110a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Inferencja\n",
    "# W tej fazie podajemy do modelu dane, w kontekście których cały ten proces był wykonany. Chcemy się dowiedzieć kto, z jaką\n",
    "# szansą porzuci w niedalekiej przyszłości naszą usługę, co jest na zdjęciu lub czy na zdjęciu jest coś co nas szczególnie interesuje,\n",
    "# a może spełniamy prośbę użytkownika o wygenerowanie zabawnego tekstu życzeń urodzinowych dla najlepszego kolegi.\n",
    "\n",
    "# przykładowy przepływ z dokumentacji dask - batch prediction\n",
    "# przykład nie jest kompletny\n",
    "\n",
    "\n",
    "from dask.distributed import LocalCluster\n",
    "\n",
    "cluster = LocalCluster(processes=False)\n",
    "client = cluster.get_client()\n",
    "\n",
    "# tu możemy wykorzystać poznany już Dask Bag\n",
    "filenames = [...]\n",
    "\n",
    "def predict(filename, model):\n",
    "    data = load(filename)\n",
    "    result = model.predict(data)\n",
    "    return result\n",
    "\n",
    "model = client.submit(load_model, path_to_model)\n",
    "predictions = client.map(predict, filenames, model=model)\n",
    "# czekamy na wszystkie wyniki\n",
    "results = client.gather(predictions)\n",
    "\n",
    "# lub wykorzystując przykład z lab_3 z użyciem dask.distributed.as_completed możemy odbierać wyniki paczkami i przetwarzać je dalej"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd7053b-4dab-43b1-a072-0e035ec42c50",
   "metadata": {},
   "source": [
    "**Przykład 5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e783491-accc-4895-a1b7-f8dd08b090f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skrypt pokazujący jak można wykorzystać Dask do rozproszonego\n",
    "# poszukiwania najbardziej optymalnych hiperparametrów danego klasyfikatora z wybranymi danymi\n",
    "# Dzięki temu możemy na niewielkiej próbce danych (ale reprezentatywnej) dobrać\n",
    "# hiperparametry modelu i przejść do szkolenia modelu docelowego na większej ilości danych\n",
    "\n",
    "from dask_ml.model_selection import IncrementalSearchCV\n",
    "import numpy as np\n",
    "from dask_ml.datasets import make_classification\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "\n",
    "X, y = make_classification(n_samples=5000000, n_features=20,\n",
    "                           chunks=100000, random_state=56)\n",
    "\n",
    "\n",
    "model = SGDClassifier(tol=1e-3, penalty='elasticnet', random_state=0)\n",
    "\n",
    "params = {'alpha': np.logspace(-2, 1, num=1000),\n",
    "          'l1_ratio': np.linspace(0, 1, num=1000),\n",
    "          'average': [True, False]}\n",
    "\n",
    "# search = IncrementalSearchCV(model, params, random_state=0)\n",
    "\n",
    "search = IncrementalSearchCV(model, params, random_state=0,\n",
    "                             n_initial_parameters=1000,\n",
    "                             patience=20, max_iter=100)\n",
    "search.fit(X, y, classes=[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1246fe-bb73-459e-8778-94ad200e2c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# najlepszy model oraz najlepsze parametry\n",
    "# więcej o tym przykładzie: \n",
    "# https://ml.dask.org/modules/generated/dask_ml.model_selection.IncrementalSearchCV.html#dask_ml.model_selection.IncrementalSearchCV\n",
    "\n",
    "search.best_score_, search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f95920-c18b-4a20-8e35-9f64a4404e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e5b1dd-95a5-4954-bafc-c70e9a251bcc",
   "metadata": {},
   "source": [
    "### Zadania"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3816e3c9-d6c3-449a-9446-933b78304ebb",
   "metadata": {},
   "source": [
    "**Zadanie 1**  \n",
    "Uruchom przykład Incremental learning z punktu 2.2 (przykład 3) dobierając parametry tak, aby ilość danych do przeliczenia była większa niż sumaryczna ilość pamięci RAM workerów. Obserwuj dashboard i w razie niepowodzenia dostosuj wielkość i ilość chunków tak, aby obliczenia się wykonały na tych samych parametrach workerów. Zobacz jak wygląda struktura pamięci na workerach, czy nie dochodzi do zrzucania pamięci na dysk (zapewne będzie on wąskim gardłem, więc w menedżerze będzie widać jego mocne obciążenie). Zastanów się czy można to jakoś zoptymalizować przy dostępnych workerach i wykonaj kilka eksperymentów szukając większej wydajności i krótszego czasu wykonania całego zadania.\n",
    "\n",
    "**Zadanie 2**  \n",
    "Dokonaj serializacji modelu z zadania 1 na dysk i następnie go wczytaj ponownie tak, aby można było uruchomić na nim predykcję dla tablic X_test oraz y_test (dla użycia miar klasyfikacji) i wyświetl macierz klasyfikacji (confusion matrix).\n",
    "\n",
    "**Zadanie 3**  \n",
    "Korzystając z danych stworzonych w zadaniu 1 uruchom poszukiwanie optymalnych parametrów modelu tak jak zostało to zaprezentowane w przykładzie 5. Ta metoda powinna sama wybierać modele obiecujące i trenować je na większej liczbie danych porzucając jednocześnie modele, które nie rokują.\n",
    "Sprawdź jak wyglądają najlepsze wyliczone parametry vs. te użyte w zadaniu 1 i ewentualnie dopasuj próbkę danych jeżeli jej inicjalna wielkość nie pozwala na wykonanie zadania (zwróć uwagę na ilość i wielkość chunków w przykładzie 3 oraz 5, w tym drugim jest ich znacznie więcej, co przyspiesza poszukiwanie optymalnych parametrów)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
