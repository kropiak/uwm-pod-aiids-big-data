{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b16751d-a424-4d5e-84ea-3384f5674aee",
   "metadata": {},
   "source": [
    "# Lab 7 - PySpark i SQL, wiaderkowanie i partycjonowanie plików oraz zapis w hurtowni danych Hive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "740da863-7895-4509-8059-653273706756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dla instalacji lokalnej\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_HOME'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f233d74-5448-4b2e-b961-b3c3e64349ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# ścieżka do bazy danych hurtowni danych oraz plików\n",
    "# należy dostosować do ścieżki względnej, w której umieszczony został bieżący notebook\n",
    "warehouse_location = './data/metastore'\n",
    "\n",
    "# utworzenie sesji Spark, ze wskazaniem włączenia obsługi Hive oraz\n",
    "# lokalizacją przechowywania hurtowni danych\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"local[2]\")\\\n",
    "        .appName(\"Apache SQL and Hive\")\\\n",
    "        .config(\"spark.memory.offHeap.enabled\",\"true\")\\\n",
    "        .config(\"spark.memory.offHeap.size\",\"4g\")\\\n",
    "        .config(\"spark.executor.memory\", \"2g\") \\\n",
    "        .config(\"spark.driver.memory\", \"1g\") \\\n",
    "        .config(\"spark.driver.host\", \"localhost\")\\\n",
    "        .enableHiveSupport()\\\n",
    "        .config(\"spark.sql.warehouse.dir\", warehouse_location)\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0a72ecb-5312-43eb-a560-2a04d5aa2c30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://localhost:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Apache SQL and Hive</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[2] appName=Apache SQL and Hive>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ff70d5-c0e3-42b3-a63d-e1035c976b16",
   "metadata": {},
   "source": [
    "## 1. Spark i SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cfd5f5-af66-4d72-bfa8-b196ccaf786c",
   "metadata": {},
   "source": [
    "Spark umożliwia zarejestrowanie obiektu DataFrame jako widoku, co umożliwia korzystanie z niego w sposób bardzo zbliżony do pracy z językiem SQL. Poniżej przykład."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9b4be41-a7fd-4be0-8752-da78d9543757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dostosuj ścieżkę do pliku do swoich danych, tutaj został utworzony mniejszy zbiór niż w poprzednim labie\n",
    "df = spark.read.csv('./data/employee.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f76ff22-0cfd-4865-a03c-7eb45059c515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tworzymy widok tymczasowy w pamięci węzła\n",
    "df.createOrReplaceTempView(\"EMPLOYEE_DATA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "275b1480-7790-4e50-8fff-008b235234e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='EMPLOYEE_DATA', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wypisanie tabeli, zwróć uwagę na to, czy stworzona tabela jest tymczasowa czy trwała\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1594570a-d565-40a4-bc0c-2c748a085bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------------------+---+--------+\n",
      "| id|firstname|           lastname|age|  salary|\n",
      "+---+---------+-------------------+---+--------+\n",
      "|  1| Zbigniew|           Barański| 46| 8797.82|\n",
      "|  2|Krzysztof|           Kowalski| 33| 7441.71|\n",
      "|  3|Krzysztof|Brzęczyszczykiewicz| 60| 8502.79|\n",
      "|  4|     Adam|         Wróblewski| 36|10258.55|\n",
      "+---+---------+-------------------+---+--------+\n",
      "\n",
      "+----------+\n",
      "| firstname|\n",
      "+----------+\n",
      "|  Zbigniew|\n",
      "| Krzysztof|\n",
      "| Krzysztof|\n",
      "|      Adam|\n",
      "|   Wisława|\n",
      "|Aleksandra|\n",
      "|     Agata|\n",
      "| Krzysztof|\n",
      "|Mieczysław|\n",
      "| Krzysztof|\n",
      "+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pobranie danych jak z tabeli SQL\n",
    "spark.sql(\"Select * from EMPLOYEE_DATA limit 4\").show()\n",
    "spark.sql(\"select firstname from EMPLOYEE_DATA\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57bd2477-3289-4512-a0f7-d5b2100b4fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+-----------------+\n",
      "| firstname|count(firstname)|      avg(salary)|\n",
      "+----------+----------------+-----------------+\n",
      "|   Wisława|         2000553|7849.951081895842|\n",
      "|Mieczysław|         2000421|7849.003746451405|\n",
      "|     Agata|         2001261|7849.029105189236|\n",
      "| Krzysztof|         1999791|7850.056622832048|\n",
      "|     Marek|         2000754|7851.120718589093|\n",
      "|      Adam|         1997466|7849.711405380585|\n",
      "| Katarzyna|         2001252|  7850.3247199353|\n",
      "|  Wojciech|         2000286|7851.790970511204|\n",
      "|  Zbigniew|         1999992|7850.794013165997|\n",
      "|Aleksandra|         1998224|7849.870652984829|\n",
      "+----------+----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select firstname, count(firstname), avg(salary) from EMPLOYEE_DATA group by firstname\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4013cf1d-d6e9-4939-badf-e1512bac553d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+--------+----------+\n",
      "|firstname|           lastname|  salary|with_raise|\n",
      "+---------+-------------------+--------+----------+\n",
      "| Zbigniew|           Barański| 8797.82|    9677.6|\n",
      "|Krzysztof|           Kowalski| 7441.71|   8185.88|\n",
      "|Krzysztof|Brzęczyszczykiewicz| 8502.79|   9353.07|\n",
      "|     Adam|         Wróblewski|10258.55|   11284.4|\n",
      "|  Wisława|           Barański|  9006.9|   9907.59|\n",
      "+---------+-------------------+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "the_raise = 0.1 # 10% podwyżki\n",
    "spark.sql(f\"select firstname, lastname, salary, round(salary + salary * {the_raise},2) as with_raise from EMPLOYEE_DATA\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43eba1d3-40b7-48bb-a4c6-1aacbe852b80",
   "metadata": {},
   "source": [
    "## 2. Apache Hive\n",
    "\n",
    "https://hive.apache.org/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e306da2c-c57f-4c05-bde9-7e6d30a203a9",
   "metadata": {},
   "source": [
    "Apache Hive, który pierwotnie został stworzony w 2007 przez Facebooka, a następnie w 2008 przekazany pod skrzydła Apache Foundation, jest nazywany hurtownią danych. Dane przechowywane są głównie w systemie **HDFS** (**Hadoop Distributed File System**), ale Hive integruje się również z innymi silnikami baz danych.\n",
    "\n",
    "Dostęp do danych jest realizowany przez **Hive QL**, który bardzo przypomina język SQL i taki sposób obsługi różnorodnych danych był jedną z głównych motywacji powstania Hive.\n",
    "\n",
    "Za pomocą zapytań Hive QL (HQL) można wykonać takie zapytania jak:\n",
    "* tworzenie i zmiana struktur tabel,\n",
    "* import i export danych,\n",
    "* agregacja danych, filtrowanie i złączenia danych.\n",
    "\n",
    "Apache Hive jest wykorzystywany w dużych ekosystemach i mimo wymienionych wyżej zalet posiada również kilka ograniczeń:\n",
    "* opóźnienie w czasie przetwarzania ze zwględu na wsadową naturę przetwarzania,\n",
    "* brak możliwości przetwarzania real-time,\n",
    "* język HQL nie daje możliwości wykonania takich operacji jak modyfikacja danych na poziomie wiersza,\n",
    "* brak możliwości przeprowadzenia zaawansowanych analiz jak współczesne nowoczesne bazy SQL.\n",
    "\n",
    "Alternatywne technologie:\n",
    "\n",
    "* Presto\n",
    "* Snowflake\n",
    "* Apache Impala\n",
    "* IBM Db2\n",
    "* Google BigQuery\n",
    "* Amazon Redshift\n",
    "* ClickHouse\n",
    "* Apache Hadoop\n",
    "* Apache HBase\n",
    "* Oracle Exadata\n",
    "* Teradata Vantage\n",
    "* Cloudera Impala"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84cdad0-562d-461a-acf0-e7aac125bc6d",
   "metadata": {},
   "source": [
    "### 2.1 Hive QL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187a784b-be51-4458-a45d-86f17e84c318",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T20:06:44.191527Z",
     "iopub.status.busy": "2024-11-19T20:06:44.190880Z",
     "iopub.status.idle": "2024-11-19T20:06:44.204303Z",
     "shell.execute_reply": "2024-11-19T20:06:44.201620Z",
     "shell.execute_reply.started": "2024-11-19T20:06:44.191477Z"
    }
   },
   "source": [
    "> Dokumentacja Apache Hive QL (dość archaiczna) jest dostępna pod adresem: https://cwiki.apache.org/confluence/display/Hive/LanguageManual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56dfdd76-9691-44ca-8d7c-a44bccd443fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spark_catalog'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentCatalog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0782ea53-1963-46be-b135-524096d06151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dla zrealizowania kolejnych przykładów dokonamy kilku modyfikacji pliku employee\n",
    "# 1. dodanie kolumny ID - indeksu\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "df = df.withColumn(\"ID\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e38ec69-5916-4619-9210-fd543a9a1fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-------------------+---+--------+\n",
      "| ID| firstname|           lastname|age|  salary|\n",
      "+---+----------+-------------------+---+--------+\n",
      "|  0|  Zbigniew|           Barański| 46| 8797.82|\n",
      "|  1| Krzysztof|           Kowalski| 33| 7441.71|\n",
      "|  2| Krzysztof|Brzęczyszczykiewicz| 60| 8502.79|\n",
      "|  3|      Adam|         Wróblewski| 36|10258.55|\n",
      "|  4|   Wisława|           Barański| 43|  9006.9|\n",
      "|  5|Aleksandra|         Wróblewski| 38| 8796.75|\n",
      "|  6|     Agata|               Glut| 64| 9252.93|\n",
      "|  7| Krzysztof|             Wlotka| 58| 8470.38|\n",
      "|  8|Mieczysław|              Pysla| 51|10216.48|\n",
      "|  9| Krzysztof|Brzęczyszczykiewicz| 48| 7853.63|\n",
      "+---+----------+-------------------+---+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25b71a3d-6bc7-43a6-a5c6-802cdd4f67fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dokonamy podziału danych i zapisania w różnych formatach\n",
    "splits = df.randomSplit(weights=[0.3, 0.7], seed=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "341b5b7d-78bb-425a-bc27-92678c7d455b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5998745, 14001255)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits[0].count(), splits[1].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe26e1b-3b69-463e-97b9-4177b68d6eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to dość dziwne zjawisko niezbyt równego podziału danych jest opisane w artykułach:\n",
    "# https://medium.com/udemy-engineering/pyspark-under-the-hood-randomsplit-and-sample-inconsistencies-examined-7c6ec62644bc\n",
    "# oraz\n",
    "# https://www.geeksforgeeks.org/pyspark-randomsplit-and-sample-methods/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24e74751-9e52-4404-ac50-059f9a361a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# większa część trafi do nowej tymczasowej tabeli\n",
    "splits[1].createOrReplaceTempView(\"EMPLOYEE_DATA_SPLIT_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe15e302-51a3-4674-9087-0c9b61019a0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='EMPLOYEE_DATA', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='EMPLOYEE_DATA_SPLIT_1', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8425355d-865a-4bda-8d48-d6ef2351a5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a mniejsza do plików JSON\n",
    "splits[0].write.json('./data/json/employee', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b1af32c-a2ce-4d10-9ef9-7b8471a6d98a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/json/employee/part-00000-a64d7312-c16a-4c93-b286-0301d9ceda44-c000.json\n",
      "./data/json/employee/part-00001-a64d7312-c16a-4c93-b286-0301d9ceda44-c000.json\n",
      "./data/json/employee/part-00002-a64d7312-c16a-4c93-b286-0301d9ceda44-c000.json\n",
      "./data/json/employee/part-00003-a64d7312-c16a-4c93-b286-0301d9ceda44-c000.json\n",
      "./data/json/employee/part-00004-a64d7312-c16a-4c93-b286-0301d9ceda44-c000.json\n",
      "./data/json/employee/part-00005-a64d7312-c16a-4c93-b286-0301d9ceda44-c000.json\n"
     ]
    }
   ],
   "source": [
    "!ls ./data/json/employee/*.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "29b41920-f283-4e0b-9b0c-050bcde0fc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aby móc wykorzystać dane w przykładach ze złączaniem, zapiszemy jeszcze próbkę danych z głównej ramki\n",
    "# z identyfikatorami oraz dodatkową kolumną z podwyżką\n",
    "from pyspark.sql.functions import col, lit, round\n",
    "\n",
    "lucky_guys = spark.sql(\"select * from EMPLOYEE_DATA\").sample(0.01)\\\n",
    ".withColumn('raise', lit('10%')).withColumn('salary after raise', round(col('salary') * 1.1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3139791d-3262-4eaa-839d-c018fca4d5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zapisujemy szczęściarzy do oddzielnej tabeli w hurtowni\n",
    "lucky_guys.write.mode('overwrite').saveAsTable(\"lucky_employees\", format='parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4bd56d3d-815d-434f-a4fc-882601e421bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='lucky_employees', catalog='spark_catalog', namespace=['default'], description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='EMPLOYEE_DATA', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='EMPLOYEE_DATA_SPLIT_1', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62071a63-692e-4102-895a-8105349a2f4c",
   "metadata": {},
   "source": [
    "#### Złączenie danych z różnych źródeł"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6facb924-c66d-47b9-8ab7-de13b33a404c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/metastore/lucky_employees/part-00000-a4cbca38-d881-4932-8d1a-0c6080487897-c000.snappy.parquet\n",
      "./data/metastore/lucky_employees/part-00001-a4cbca38-d881-4932-8d1a-0c6080487897-c000.snappy.parquet\n",
      "./data/metastore/lucky_employees/part-00002-a4cbca38-d881-4932-8d1a-0c6080487897-c000.snappy.parquet\n",
      "./data/metastore/lucky_employees/part-00003-a4cbca38-d881-4932-8d1a-0c6080487897-c000.snappy.parquet\n",
      "./data/metastore/lucky_employees/part-00004-a4cbca38-d881-4932-8d1a-0c6080487897-c000.snappy.parquet\n",
      "./data/metastore/lucky_employees/part-00005-a4cbca38-d881-4932-8d1a-0c6080487897-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls ./data/metastore/lucky_employees/*.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4082ccce-86f4-4bc9-b37f-71c37f09ec32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-------------------+-------+-----+------------------+\n",
      "|   ID| firstname|           lastname| salary|raise|salary after raise|\n",
      "+-----+----------+-------------------+-------+-----+------------------+\n",
      "| 2764|      Adam|              Pysla|8391.06|  10%|           9230.17|\n",
      "| 8037|  Zbigniew|             Wlotka| 6483.3|  10%|           7131.63|\n",
      "|11574|Mieczysław|              Pysla|8753.58|  10%|           9628.94|\n",
      "|12300|  Wojciech|               Glut|9688.33|  10%|          10657.16|\n",
      "|20584|   Wisława|           Kowalski|8093.44|  10%|           8902.78|\n",
      "|22784| Krzysztof|           Kowalski|6608.03|  10%|           7268.83|\n",
      "|26282|     Marek|              Pysla|8264.48|  10%|           9090.93|\n",
      "|27120|     Marek|             Szczaw|8055.54|  10%|           8861.09|\n",
      "|31326|  Wojciech|Brzęczyszczykiewicz|7976.06|  10%|           8773.67|\n",
      "|32890| Katarzyna|           Barański|8956.59|  10%|           9852.25|\n",
      "+-----+----------+-------------------+-------+-----+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# przykład złączania danych na różnych źródłach\n",
    "# zapytanie SQL bezpośrednio na plikach - tutaj zapisanych wcześniej JSON-ach oraz parquet\n",
    "query = \"\"\"\n",
    "SELECT ed.ID, ed.firstname, ed.lastname, ed.salary, lucky.raise, lucky.`salary after raise`\n",
    "FROM json.`./data/json/employee/` as jtable \n",
    "join EMPLOYEE_DATA ed on jtable.ID=ed.ID \n",
    "join parquet.`./data/metastore/lucky_employees/` as lucky on ed.ID=lucky.ID\n",
    "\"\"\"\n",
    "df_from_json = spark.sql(query).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19de17c-1ff5-4f4f-8cff-f961cfa1346f",
   "metadata": {},
   "source": [
    "#### Dzielenie danych na wiaderka (ang. buckets) i partycje"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba3ad9f-9c7b-460e-832c-6960e431f896",
   "metadata": {},
   "source": [
    "Dzielenie danych na wiaderka jest rozwiązaniem, które stosowane jest do podziału danych na mniejsze części w sposób, który może przyspieszyć obliczenia poprzez zredukowanie liczby operacji przetasowania danych (ang. shuffle, a w kontekście Sparka mówimy o operacji exchange), które są bardzo kosztowne, gdyż wykonywane są często między węzłami (workerami)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ab4b1370-d4dc-4a73-8a7a-61c7c0794f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ten przykład pokazuje podział na 16 wiaderek danych bazując na podziale po kolumnie ID (tu używane jest hashowanie)\n",
    "# dane posortowane są w każdym buckecie po kolumnie salary\n",
    "# dane zapisywane są do hurtowni Hive, a informacje o zapisanych tam tabelach przechowywane są w\n",
    "# Hive metastore (domyślnie jest do baza danych Derby)\n",
    "df.write.bucketBy(16, 'ID').mode('overwrite').sortBy('salary').saveAsTable('employee_id_bucketed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0d6aa48c-3abf-415d-b87e-fb0ff6258fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/metastore/employee_id_bucketed/part-00000-2626a2cb-d44d-4e75-855e-7fc33daf774d_00000.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00000-2626a2cb-d44d-4e75-855e-7fc33daf774d_00001.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00000-2626a2cb-d44d-4e75-855e-7fc33daf774d_00002.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00000-2626a2cb-d44d-4e75-855e-7fc33daf774d_00003.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00000-2626a2cb-d44d-4e75-855e-7fc33daf774d_00004.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00000-2626a2cb-d44d-4e75-855e-7fc33daf774d_00005.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00000-2626a2cb-d44d-4e75-855e-7fc33daf774d_00006.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00000-2626a2cb-d44d-4e75-855e-7fc33daf774d_00007.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00000-2626a2cb-d44d-4e75-855e-7fc33daf774d_00008.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00000-2626a2cb-d44d-4e75-855e-7fc33daf774d_00009.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00000-2626a2cb-d44d-4e75-855e-7fc33daf774d_00010.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00000-2626a2cb-d44d-4e75-855e-7fc33daf774d_00011.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00000-2626a2cb-d44d-4e75-855e-7fc33daf774d_00012.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00000-2626a2cb-d44d-4e75-855e-7fc33daf774d_00013.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00000-2626a2cb-d44d-4e75-855e-7fc33daf774d_00014.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00000-2626a2cb-d44d-4e75-855e-7fc33daf774d_00015.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00001-2626a2cb-d44d-4e75-855e-7fc33daf774d_00000.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00001-2626a2cb-d44d-4e75-855e-7fc33daf774d_00001.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00001-2626a2cb-d44d-4e75-855e-7fc33daf774d_00002.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00001-2626a2cb-d44d-4e75-855e-7fc33daf774d_00003.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00001-2626a2cb-d44d-4e75-855e-7fc33daf774d_00004.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00001-2626a2cb-d44d-4e75-855e-7fc33daf774d_00005.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00001-2626a2cb-d44d-4e75-855e-7fc33daf774d_00006.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00001-2626a2cb-d44d-4e75-855e-7fc33daf774d_00007.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00001-2626a2cb-d44d-4e75-855e-7fc33daf774d_00008.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00001-2626a2cb-d44d-4e75-855e-7fc33daf774d_00009.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00001-2626a2cb-d44d-4e75-855e-7fc33daf774d_00010.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00001-2626a2cb-d44d-4e75-855e-7fc33daf774d_00011.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00001-2626a2cb-d44d-4e75-855e-7fc33daf774d_00012.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00001-2626a2cb-d44d-4e75-855e-7fc33daf774d_00013.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00001-2626a2cb-d44d-4e75-855e-7fc33daf774d_00014.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00001-2626a2cb-d44d-4e75-855e-7fc33daf774d_00015.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00002-2626a2cb-d44d-4e75-855e-7fc33daf774d_00000.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00002-2626a2cb-d44d-4e75-855e-7fc33daf774d_00001.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00002-2626a2cb-d44d-4e75-855e-7fc33daf774d_00002.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00002-2626a2cb-d44d-4e75-855e-7fc33daf774d_00003.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00002-2626a2cb-d44d-4e75-855e-7fc33daf774d_00004.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00002-2626a2cb-d44d-4e75-855e-7fc33daf774d_00005.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00002-2626a2cb-d44d-4e75-855e-7fc33daf774d_00006.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00002-2626a2cb-d44d-4e75-855e-7fc33daf774d_00007.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00002-2626a2cb-d44d-4e75-855e-7fc33daf774d_00008.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00002-2626a2cb-d44d-4e75-855e-7fc33daf774d_00009.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00002-2626a2cb-d44d-4e75-855e-7fc33daf774d_00010.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00002-2626a2cb-d44d-4e75-855e-7fc33daf774d_00011.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00002-2626a2cb-d44d-4e75-855e-7fc33daf774d_00012.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00002-2626a2cb-d44d-4e75-855e-7fc33daf774d_00013.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00002-2626a2cb-d44d-4e75-855e-7fc33daf774d_00014.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00002-2626a2cb-d44d-4e75-855e-7fc33daf774d_00015.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00003-2626a2cb-d44d-4e75-855e-7fc33daf774d_00000.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00003-2626a2cb-d44d-4e75-855e-7fc33daf774d_00001.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00003-2626a2cb-d44d-4e75-855e-7fc33daf774d_00002.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00003-2626a2cb-d44d-4e75-855e-7fc33daf774d_00003.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00003-2626a2cb-d44d-4e75-855e-7fc33daf774d_00004.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00003-2626a2cb-d44d-4e75-855e-7fc33daf774d_00005.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00003-2626a2cb-d44d-4e75-855e-7fc33daf774d_00006.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00003-2626a2cb-d44d-4e75-855e-7fc33daf774d_00007.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00003-2626a2cb-d44d-4e75-855e-7fc33daf774d_00008.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00003-2626a2cb-d44d-4e75-855e-7fc33daf774d_00009.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00003-2626a2cb-d44d-4e75-855e-7fc33daf774d_00010.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00003-2626a2cb-d44d-4e75-855e-7fc33daf774d_00011.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00003-2626a2cb-d44d-4e75-855e-7fc33daf774d_00012.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00003-2626a2cb-d44d-4e75-855e-7fc33daf774d_00013.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00003-2626a2cb-d44d-4e75-855e-7fc33daf774d_00014.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00003-2626a2cb-d44d-4e75-855e-7fc33daf774d_00015.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00004-2626a2cb-d44d-4e75-855e-7fc33daf774d_00000.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00004-2626a2cb-d44d-4e75-855e-7fc33daf774d_00001.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00004-2626a2cb-d44d-4e75-855e-7fc33daf774d_00002.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00004-2626a2cb-d44d-4e75-855e-7fc33daf774d_00003.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00004-2626a2cb-d44d-4e75-855e-7fc33daf774d_00004.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00004-2626a2cb-d44d-4e75-855e-7fc33daf774d_00005.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00004-2626a2cb-d44d-4e75-855e-7fc33daf774d_00006.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00004-2626a2cb-d44d-4e75-855e-7fc33daf774d_00007.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00004-2626a2cb-d44d-4e75-855e-7fc33daf774d_00008.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00004-2626a2cb-d44d-4e75-855e-7fc33daf774d_00009.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00004-2626a2cb-d44d-4e75-855e-7fc33daf774d_00010.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00004-2626a2cb-d44d-4e75-855e-7fc33daf774d_00011.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00004-2626a2cb-d44d-4e75-855e-7fc33daf774d_00012.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00004-2626a2cb-d44d-4e75-855e-7fc33daf774d_00013.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00004-2626a2cb-d44d-4e75-855e-7fc33daf774d_00014.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00004-2626a2cb-d44d-4e75-855e-7fc33daf774d_00015.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00005-2626a2cb-d44d-4e75-855e-7fc33daf774d_00000.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00005-2626a2cb-d44d-4e75-855e-7fc33daf774d_00001.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00005-2626a2cb-d44d-4e75-855e-7fc33daf774d_00002.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00005-2626a2cb-d44d-4e75-855e-7fc33daf774d_00003.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00005-2626a2cb-d44d-4e75-855e-7fc33daf774d_00004.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00005-2626a2cb-d44d-4e75-855e-7fc33daf774d_00005.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00005-2626a2cb-d44d-4e75-855e-7fc33daf774d_00006.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00005-2626a2cb-d44d-4e75-855e-7fc33daf774d_00007.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00005-2626a2cb-d44d-4e75-855e-7fc33daf774d_00008.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00005-2626a2cb-d44d-4e75-855e-7fc33daf774d_00009.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00005-2626a2cb-d44d-4e75-855e-7fc33daf774d_00010.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00005-2626a2cb-d44d-4e75-855e-7fc33daf774d_00011.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00005-2626a2cb-d44d-4e75-855e-7fc33daf774d_00012.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00005-2626a2cb-d44d-4e75-855e-7fc33daf774d_00013.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00005-2626a2cb-d44d-4e75-855e-7fc33daf774d_00014.c000.snappy.parquet\n",
      "./data/metastore/employee_id_bucketed/part-00005-2626a2cb-d44d-4e75-855e-7fc33daf774d_00015.c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls ./data/metastore/employee_id_bucketed/*.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d119e520-0a46-4769-9c4c-a0cae74d3001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-------------------+---+-------+\n",
      "|     ID|firstname|           lastname|age| salary|\n",
      "+-------+---------+-------------------+---+-------+\n",
      "|2422785|     Adam|Brzęczyszczykiewicz| 55|3121.65|\n",
      "|1580567|Krzysztof|Brzęczyszczykiewicz| 41|3547.67|\n",
      "|3397177|    Marek|Brzęczyszczykiewicz| 20|3630.98|\n",
      "| 974730| Zbigniew|       Mieczykowski| 59|3640.82|\n",
      "|1505525|    Agata|         Malinowski| 62|3681.82|\n",
      "| 153754|Krzysztof|       Mieczykowski| 27|3736.18|\n",
      "|2775459|  Wisława|Brzęczyszczykiewicz| 65|3755.73|\n",
      "|1648207|     Adam|       Mieczykowski| 38|3765.32|\n",
      "|1296355|    Marek|           Barański| 25|3783.08|\n",
      "|2188892|     Adam|       Mieczykowski| 18|3785.55|\n",
      "+-------+---------+-------------------+---+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.table('employee_id_bucketed').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "42459d91-1ac8-45fe-a703-36868e5bb78d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='employee_id_bucketed', catalog='spark_catalog', namespace=['default'], description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='lucky_employees', catalog='spark_catalog', namespace=['default'], description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='EMPLOYEE_DATA', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='EMPLOYEE_DATA_SPLIT_1', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wypisanie tabeli\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3b776873-29ba-4003-9e5a-40102584a028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# usunięcie tabeli\n",
    "spark.sql('DROP TABLE employee_id_bucketed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "64898821-0059-4638-8f80-f54c980e4166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='lucky_employees', catalog='spark_catalog', namespace=['default'], description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='EMPLOYEE_DATA', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='EMPLOYEE_DATA_SPLIT_1', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wypisanie tabeli\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "72a3ec83-e800-4e74-9c9b-18bf06374cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jeżeli dane, z którymi pracujemy zawierają stosunkowo niewiele różnorodnych wartości w danych kolumnach\n",
    "# lub filtrowanie i obliczenia często odbywają się na podgrupach danych to lepsze efekty uzyskamy\n",
    "# poprzez wykorzystanie możliwości partycjonowania tych danych, które to partycjonowanie\n",
    "# będzie również odzwierciedlone w fizycznej strukturze plików na dysku twardym w hurtowni danych\n",
    "\n",
    "# zobaczmy przykład poniżej\n",
    "\n",
    "df.write.partitionBy(\"lastname\").mode('overwrite').saveAsTable(\"employees_partitioned_lastname\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "01e6899a-aae3-42cd-9daf-f372150bc551",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dobrym pomysłem jest też określenie ilości bucketów wynikających z danych w konkretnej kolumnie\n",
    "# i wykorzystanie do podziału\n",
    "# https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.bucketBy.html\n",
    "buckets = spark.sql(\"select distinct firstname from EMPLOYEE_DATA\").count()\n",
    "buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "35217863-794b-4ae9-9fda-5ec6341c519d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_SUCCESS\n",
      "lastname=BaraĹ„ski\n",
      "lastname=BrzÄ™czyszczykiewicz\n",
      "lastname=Glut\n",
      "lastname=Kowalski\n",
      "lastname=Malinowski\n",
      "lastname=Mieczykowski\n",
      "lastname=Pysla\n",
      "lastname=Szczaw\n",
      "lastname=Wlotka\n",
      "lastname=WrĂłblewski\n"
     ]
    }
   ],
   "source": [
    "# widok danych podzielonych na partycję z punktu widzenia systemu plików\n",
    "!ls ./data/metastore/employees_partitioned_lastname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a9eb0165-7e9f-42a4-a161-11ddb6c06925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[lastname#19], functions=[avg(salary#21)])\n",
      "   +- Exchange hashpartitioning(lastname#19, 200), ENSURE_REQUIREMENTS, [plan_id=820]\n",
      "      +- HashAggregate(keys=[lastname#19], functions=[partial_avg(salary#21)])\n",
      "         +- Filter (isnotnull(lastname#19) AND (lastname#19 = Pysla))\n",
      "            +- FileScan csv [lastname#19,salary#21] Batched: false, DataFilters: [isnotnull(lastname#19), (lastname#19 = Pysla)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/C:/Users/Krzysztof/__projects/__pyspark_local_aiids/data/employe..., PartitionFilters: [], PushedFilters: [IsNotNull(lastname), EqualTo(lastname,Pysla)], ReadSchema: struct<lastname:string,salary:double>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.lastname == 'Pysla').groupby('lastname').agg({'salary': 'avg'}).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "721dc597-bc44-4573-bd78-ef6cddf1f403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+\n",
      "|lastname|      avg(salary)|\n",
      "+--------+-----------------+\n",
      "|   Pysla|7848.517332754915|\n",
      "+--------+-----------------+\n",
      "\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 10.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df.filter(df.lastname == 'Pysla').groupby('lastname').agg({'salary': 'avg'}).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d4881a6b-8b57-4f42-b596-645ba485a5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[lastname#684], functions=[avg(salary#683)])\n",
      "   +- Exchange hashpartitioning(lastname#684, 200), ENSURE_REQUIREMENTS, [plan_id=886]\n",
      "      +- HashAggregate(keys=[lastname#684], functions=[partial_avg(salary#683)])\n",
      "         +- FileScan parquet spark_catalog.default.employees_partitioned_lastname[salary#683,lastname#684] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/C:/Users/Krzysztof/__projects/__pyspark_local_aiids/data/metasto..., PartitionFilters: [isnotnull(lastname#684), (lastname#684 = Pysla)], PushedFilters: [], ReadSchema: struct<salary:double>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select lastname, avg(salary) from employees_partitioned_lastname where lastname='Pysla' group by lastname\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "faed71dd-9206-4cf1-b4d9-3a1771e7a2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+\n",
      "|lastname|      avg(salary)|\n",
      "+--------+-----------------+\n",
      "|   Pysla|7848.517332754809|\n",
      "+--------+-----------------+\n",
      "\n",
      "CPU times: total: 15.6 ms\n",
      "Wall time: 317 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "spark.sql(\"select lastname, avg(salary) from employees_partitioned_lastname where lastname='Pysla' group by lastname\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4091f3b3-4ea6-4211-bfc5-9509b3a2fef5",
   "metadata": {},
   "source": [
    "Jak widać, operacja wykonała się szybciej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c27b5bda-2b0a-4709-82ba-05dafc721131",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bb2215-9073-4480-8bf2-5618dd6a954a",
   "metadata": {},
   "source": [
    "### Zadania"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3a43c2-4594-4ffe-a0aa-0f3fc028fba8",
   "metadata": {},
   "source": [
    "**Zadanie 1**  \n",
    "Pamiętacie plik zamówienia.txt ?\n",
    "Plik został umieszczony w folderze z labem w repozytorium.\n",
    "\n",
    "Wczytaj ten plik za pomocą Sparka do dowolnego typu danych (RDD, Spark DataFrame) i dokonaj transformacji tak aby:\n",
    "* naprawić problemy z kodowaniem znaków (replace?) w kolumnie Sprzedawca\n",
    "* poprawić format danych w kolumnie Utarg\n",
    "* dodać odpowiednie typy danych\n",
    "* kolumna idZamowienia powinna być traktowana jako klucz (indeks)\n",
    "\n",
    "**Zadanie 2**  \n",
    "Po wykonaniu zadania 1, wykorzystaj przykłady z laboratorium i:\n",
    "* 2.1 wykonaj wiaderkowanie danych i wykonaj dowolne zapytanie agregujące na tych danych vs. dane niepodzielone na wiaderka - porównaj czas\n",
    "* 2.2 wykonaj partycjonowanie danych i zapisz je w formcie csv (wypróbuj partycjonowanie wg. kraju, nazwiska\n",
    "* 2.3 wykonaj zapytanie agregujące z filtrowanie po kolumnie, której użyłeś/-aś do partycjonowania na danych oryginalnych oraz partycjonowanych i porównaj czas wykonania\n",
    "\n",
    "**Zadanie 3**  \n",
    "Z danych wygeneruj 4 różne podzbiory próbek (wiersze wybrane losowo) i dodaj nową kolumnę w każdym z nich, np. w jednym stwórz kolumnę month wyciągając tylko miesiąc z daty, w drugim wartość netto zamówienia (przyjmując, że vat to 23%), w kolejnym zamień nazwisko na wielkie litery, w kolejnym dodaj kolumnę waluta z wartością PLN.\n",
    "\n",
    "Następnie zapisz każdy z tych zbiorów tak, że:\n",
    "* zbiór pierwszy to będzie tymczasowa tabela in-memory Sparka\n",
    "* zbiór drugi to plik(i) parquet\n",
    "* zbiór trzeci to plik(i) csv\n",
    "* zbiór czwarty to plik(i) json\n",
    "\n",
    "Wykonaj zapytanie złączające jak w przykładzie pobierając dane bezpośrednio z plików i wyświetl idZamowienia, Kraj, Sprzedawcę, Datę, Utarg oraz 4 nowo utworzone kolumny.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
