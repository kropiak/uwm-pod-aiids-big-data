{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fa05bbf-4cf7-4e10-ba10-db527e9176d6",
   "metadata": {},
   "source": [
    "# Lab 5 - Apache Spark - wprowadzenie."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50d92ad-8fc0-468f-9e51-fe5bceed2fb1",
   "metadata": {},
   "source": [
    "Apache Spark jest silnikiem do przetwarzania dancych na dużą skalę, pozwalający na wykonywanie operacji w sposób zrównoleglony i rozproszony. Spark dostarcza API dla języków java, Scala, Python oraz R do przetwarzania grafów obliczeń. Spark składa się z wielu narzędzi takich jak:\n",
    "* Resilient Distributed Datasets (RDD) - niskopoziomowy typ zbioru danych Spark, na którym opierają się struktury danych na wyższych poziomach abstrakcji,\n",
    "* Spark SQL - Spark Dataset, Spark DataFrame,\n",
    "* Pandas API on Spark - API pozwalające na wykorzystanie biblioteki pandas w sposób zrównoleglony na klastrze Spark,\n",
    "* Structured Streaming - zestaw narzędzi do przetwarzania strumieniowego,\n",
    "* MLlib - moduł wspierający wykorzystanie Machine Learning z użyciem typów danych Spark oraz klastrów Spark,\n",
    "* GraphX - przetwarzanie grafów,\n",
    "* SparkR - API w języku R do pracy w środowisku Spark,\n",
    "* PySpark - API Python do pracy w środowisku Spark,\n",
    "* Spark SQL CLI - przetwarzanie danych z użyciem Spark SQL z poziomu wiersza poleceń."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b99b46f-13ca-4762-9858-d3b201e670f8",
   "metadata": {},
   "source": [
    "## 1. Uruchomienie środowiska Spark z wykorzystaniem PySpark.\n",
    "\n",
    "W trakcie zajęc z racji dotychczasowego środowiska pracy (Python) będzie wykorzystywane API Pythona, które dostarcza Spark.\n",
    "\n",
    "**Dokumentacja Spark Python API:** https://spark.apache.org/docs/3.5.5/api/python/index.html\n",
    "\n",
    "> Oficjalna dokumentacja alternatywnych sposobów instalacji oraz zależności modułu PySpark: https://spark.apache.org/docs/latest/api/python/getting_started/install.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53102ad-a0cc-48d7-8dbf-561f91f85e63",
   "metadata": {},
   "source": [
    "**UWAGA!**\n",
    "\n",
    "Uruchamiamy poniższą komórkę tylko dla konfiguracji z Dockerem!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97b77cf-2391-4f51-b98a-9002bc22d597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['SPARK_NAME'] = \"/opt/spark\"\n",
    "# os.environ['PYSPARK_DRIVER_PYTHON'] = 'jupyter'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS'] = 'lab'\n",
    "# os.environ['PYSPARK_PYTHON'] = 'python'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = '/opt/spark/work-dir/.venv/bin/python3'\n",
    "os.environ['PYSPARK_PYTHON'] = '/opt/spark/work-dir/.venv/bin/python3'\n",
    "\n",
    "# można też spróbować wykorzystać moduł findspark do automatycznego odnalezienia miejsca instalacji sparka\n",
    "# import findspark\n",
    "# findspark.init()\n",
    "# lub\n",
    "# findspark.init(\"/opt/spark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e497cfc-c0a2-4d5d-8f97-d0269f44b3aa",
   "metadata": {},
   "source": [
    "**UWAGA!**\n",
    "\n",
    "Uruchamiamy poniższą komórkę tylko dla konfiguracji z lokalną instalacją PySpark (scenariusz 1)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe0f878b-bb8c-4b61-ae9d-3845318bb0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_HOME'] = sys.executable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a1abfd-62dd-41ff-99f1-66c055883a93",
   "metadata": {},
   "source": [
    "> Przed uruchomieniem poniższej komórki należy zainstalować moduł `pyspark` do środowiska wirtualnego Pythona"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efbb0fb-ebe5-4450-bb02-8428f5e3d3ee",
   "metadata": {},
   "source": [
    "> Oficjana dokumentacja konfiguracji Sparka, również parametrów wywołania: https://spark.apache.org/docs/3.5.5/configuration.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14aaa31-0a62-4890-9646-74cea29e2e1a",
   "metadata": {},
   "source": [
    "Aby możliwe było wysłanie zadań do wykonania z wykorzystaniem klastra Spark (lokalnego lub rozproszonego) musimy uzyskać najpierw **referencję do obiektu typu SparkSession**. Tworząc obiekt musimy zdefiniować nazwę aplikacji, którą otrzyma ta sesja oraz możemy zdefiniować dodatkowe parametry. Przykład poniżej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b7a59b7-ad6d-43ff-b5dd-2bc48d45bd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# findspark.init()\n",
    "\n",
    "# local[2] - oznacza dwa executory (workery), można to zmienić w razie potrzeb dla nowej sesji\n",
    "# \"spark.executor.memory\", \"2g\" - 2GB RAM dla każdego executora\n",
    "# \"spark.driver.memory\", \"1g\" - 1GB RAM dla drivera\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"local[2]\")\\\n",
    "        .appName(\"Create-DataFrame\")\\\n",
    "        .config(\"spark.executor.memory\", \"2g\") \\\n",
    "        .config(\"spark.driver.memory\", \"1g\") \\\n",
    "        .config(\"spark.memory.offHeap.enabled\",\"true\")\\\n",
    "        .config(\"spark.memory.offHeap.size\",\"4g\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dea775c6-b11f-412b-8077-2554ac93fa92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-L9AQONII:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Create-DataFrame</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[2] appName=Create-DataFrame>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f167ac46-653d-489a-9e63-b31fae824f3d",
   "metadata": {},
   "source": [
    "Podobnie jak w przypadku biblioteki Dask możemy śledzić pracę klastra poprzez przeglądarkę. Adres to http://\\<host\\>:4040. Wykorzystując dockera wymagane jest stworzenie odpowiedniego mapowania portu oraz w naszym przypadków również zmiany domyślnego hosta, który zapewne będzie skróconą wersją hasha kontenera. Skoro port jest zmapowany to znaczy, że będzie dostępny na hoście lokalnym. Finalnie więc adres dla węzła master powinien być dostępny pod adresem `http://localhost:4040`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8565e981-0c45-4694-8244-34dcf06b1d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jeżeli chcemy zatrzymać sesję Spark (lub na potrzeby jej resetu)\n",
    "spark.sparkContext.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f834a08-53fb-456e-9c8d-ab5dca9b3a2c",
   "metadata": {},
   "source": [
    "## 2. Spark Resilient Distributed Datasets (RDD)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0646cd6f-408e-41db-ab88-29d2a1f4913c",
   "metadata": {},
   "source": [
    "Na wysokim poziomie każda aplikacja Spark składa się z **programu sterownika (ang. driver program)**, który uruchamia główną funkcję użytkownika i wykonuje różne operacje równoległe na klastrze. \n",
    "\n",
    "Główną abstrakcją zapewnianą przez Spark jest **odporny rozproszony zbiór danych (Resilient Distributed Datasets - RDD)**, który jest zbiorem elementów podzielonych na węzły klastra, które mogą być obsługiwane równolegle. RDD mogą być również utrwalane w pamięci klastra, umożliwiając jego ponowne wykorzystanie w operacjach równoległych. Wreszcie, RDD automatycznie odzyskują sprawność po awarii węzła.\n",
    "\n",
    "Wykorzystanie API RDD nie jest rekomendowane w wielu przypadkach, gdyż wymaga dobrej znajomości niskopoziomowego API Sparka i ręcznej optymalizacji operacji. Zazwyczaj lepszym pomysłem będzie wykorzystanie Spark Dataset oraz Spark DataFrame.\n",
    "\n",
    "Kilka przykładów wykorzystania tego API zostanie jednak tutaj przedstawionych.\n",
    "\n",
    "> Poradnik programisty dla RDD znajduje się pod linkiem: https://spark.apache.org/docs/3.5.5/rdd-programming-guide.html  \n",
    "> API dla PySpark RDD: https://spark.apache.org/docs/3.5.5/api/python/reference/api/pyspark.RDD.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "98272878-be6a-42ca-95df-bdfaddb12643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liczba partycji: 2\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lista wartości zostaje podzielona na partycje i rozproszona na wszystkie dostępne węzły\n",
    "# https://spark.apache.org/docs/3.5.5/api/python/reference/api/pyspark.SparkContext.parallelize.html\n",
    "rdd = spark.sparkContext.parallelize(list(range(20)))\n",
    "\n",
    "# rdd w formie rozproszonej zostaje scalone w listę zawierającą wszystkie elementy RDD\n",
    "# np. za pomocą funkcji collect()\n",
    "# https://spark.apache.org/docs/3.5.5/api/python/reference/api/pyspark.RDD.collect.html\n",
    "\n",
    "rddCollect = rdd.collect()\n",
    "display(type(rddCollect))\n",
    "print(f\"Liczba partycji: {rdd.getNumPartitions()}\")\n",
    "print(rddCollect)\n",
    "# print(f\"Pierwszy element: {rdd.first()}\")\n",
    "# print(f\"Ile elementów: {rdd.count()}\")\n",
    "rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "19fd79bf-7544-4d16-8b86-7b5dd0fabf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obiekt RDD może przechowywać dane z różnych źródeł, które są zgodne z systemem plików Apache Hadoop\n",
    "# np. Amazon S3, Cassandra, HDFS, HBase i inne\n",
    "\n",
    "# możemy dla uniknięcia potrzeby każdorazowego odwoływania się do kontekstu poprzez spark.sparkContext zapisać sobie to w zmiennej pomocniczej\n",
    "sc = spark.sparkContext\n",
    "# tutaj wczytamy do RDD plik tekstowy\n",
    "pan_tadeusz_file = sc.textFile(\"pan-tadeusz.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080ebe6e-b412-4c4a-b550-9b489a8b00e7",
   "metadata": {},
   "source": [
    "Więcej informacji odnośnie obsługi plików w środowisku Spark można znaleźć m.in. tu: https://spark.apache.org/docs/3.5.5/rdd-programming-guide.html#external-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63510143-12ef-4a56-bb50-15d993fb6e44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Adam Mickiewicz',\n",
       " '',\n",
       " 'Pan Tadeusz',\n",
       " 'czyli ostatni zajazd na Litwie',\n",
       " '',\n",
       " 'ISBN 978-83-288-2495-9',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(pan_tadeusz_file.getNumPartitions())\n",
    "\n",
    "# jaka jest struktura tego zbioru danych?\n",
    "# pojedyncza wartość to linia z pliku\n",
    "pan_tadeusz_file.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "08afe171-686f-44f9-b9dd-ddd309397aff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# możemy zmienić liczbę automatycznie stworzonych partycji i ponownie rozproszyć je po węzłach\n",
    "pan_tadeusz_file = pan_tadeusz_file.repartition(4)\n",
    "pan_tadeusz_file.getNumPartitions()\n",
    "\n",
    "# również metoda coalesce może posłużyć nam do zmiany ilości partycji dla obiektu RDD np. po zastosowaniu filtrowania, które\n",
    "# znacznie zmniejsza wielkość pierwotnego obiektu RDD a co za tym idzie każdej partycji i dalsze obliczenia mogą nie być\n",
    "# wykonywane zbyt efektywnie (zbyt mały rozmiar partycji)\n",
    "# https://spark.apache.org/docs/3.5.5/api/python/reference/api/pyspark.RDD.coalesce.html\n",
    "# główna różnica między repartition a coalesce jest taka, że ta pierwsza wykorzystuje mechanizm tasowania danych a ta druga może, ale nie\n",
    "# musi go wykorzystywać gdyż możemy tym sterować za pomocą parametru wywołania tej metody"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59fcddb4-0dcb-4820-ab88-80e7987e1079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jedną z funkcji dostępnej w tym API jest możliwość wykonania funkcji na każdej z partycji\n",
    "# minusem może być to, że funkcja foreachPartition zwraca typ None, więc wyniki należy przetworzyć w inny sposób\n",
    "# docs: https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.foreachPartition.html\n",
    "\n",
    "def count_words(iterator):\n",
    "    words = sum([len(x.split()) for x in iterator])\n",
    "    print(words)\n",
    "\n",
    "pan_tadeusz_file.foreachPartition(count_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b92b965e-e67e-4e88-857a-a929d7c4e148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w przypadku pracy w środowisku rozproszonym (cluster mode) nie zobaczymy rezultatów, gdyż zostały one wykonane na\n",
    "# executorach (tutaj funkcja print)\n",
    "# jeżeli chcemy jednak zobaczyć efekt, to należy najpierw pobrać dane na driver poprzez np. collect(), a następnie\n",
    "# wywołać funkcję, ale to nie zadziała dla PySparka, gdyż w tym API collect() zwraca obiekt typu list, na którym\n",
    "# nie możemy wywołać foreach() - to zadziałe jednak w natywnym środowisku Sparka, czyli w języku scala"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94610e50-5d24-4f09-aa82-002c89a9031e",
   "metadata": {},
   "source": [
    "Przy wypisywaniu wartości z RDD trzeba również zwrócić uwagę na różnicę w działaniu tych metod w trybie pracy lokalnej (czyli tak jak w tym labie) oraz klastra. Efekty mogą być różne, więcej przeczytasz tu: https://spark.apache.org/docs/3.5.5/rdd-programming-guide.html#printing-elements-of-an-rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5e2f7e-7da8-4cfd-bbff-7d882194392c",
   "metadata": {},
   "source": [
    "**RDD obsługują dwa rodzaje operacji: transformacje**, które tworzą nowy zbiór danych z istniejącego, oraz **akcje**, które zwracają wartość do programu sterownika po uruchomieniu obliczeń na zbiorze danych. Przykładowo, map jest transformacją, która przepuszcza każdy element zbioru danych przez funkcję i zwraca nowy RDD reprezentujący wyniki. Z drugiej strony, reduce jest akcją, która agreguje wszystkie elementy RDD przy użyciu pewnej funkcji i zwraca końcowy wynik do programu sterownika.\n",
    "\n",
    "Wszystkie transformacje są wykonywane w sposób **leniwy** tzn, że obliczenia nie są wykonywane dopóki nie jest potrzebnych wynik. To podobnie jak w przypadku frameworka Dask umożliwia optymalizację obliczeń np. w przypadku gdy nie są potrzebne wyniki pośrednie z każdego węzła po zastosowaniu funkcji poprzez `map`, ale tylo wynik akcji `reduce`, więc nie ma potrzeby przesyłania całych pośrednich RDD do drivera.\n",
    "\n",
    "**Lista wybranych transformacji dostępna jest tu:** https://spark.apache.org/docs/3.5.5/rdd-programming-guide.html#transformations\n",
    "\n",
    "**Lista wybranych akcji tu:** https://spark.apache.org/docs/3.5.5/rdd-programming-guide.html#actions\n",
    "\n",
    "W uzasadnionych przypadkach można również przyspieszyć obliczenia poprzez utrwalenie danych w pamięci lub pamięci podręcznej poprzez metody `persist` lub `cache` na obiekcie RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789f6556-df63-438f-997b-75dcab6466d8",
   "metadata": {},
   "source": [
    "**Kilka przykładów transformacji i akcji**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a4f65130-0146-4bf1-9ca7-d7d9af40e51e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[25] at RDD at PythonRDD.scala:53"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[8, 6, 7, 5, 8, 5, 6, 4, 7, 5]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "69095"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# funkcje map oraz reduce\n",
    "\n",
    "# możemy również wykonać operację w inny sposób, tym raze mapując funkcję na każdy element obiektu RDD\n",
    "# zwrócony zostanie obiekt RDD, na którym możemy wykonać kolejne operacje\n",
    "\n",
    "# transformacje są leniwe - to obiekt RDD, a nie obiekt z przeliczoną ilością wyrazów\n",
    "display(pan_tadeusz_file.map(lambda s: len(s.split())))\n",
    "\n",
    "# dzielimy każdą wartość na wyrazy (tak domyślnie zadziała split() na linii tekstu,\n",
    "# a następnie wywołujemy funkcję len(), na tym co zostanie zwrócone (ilość wyrazów)\n",
    "# wyświetlamy tylko pierwsze 10 wartości poprzez take(10) - i dopiero ta akcja faktycznie wyzwala obliczenia\n",
    "display(pan_tadeusz_file.map(lambda s: len(s.split())).take(10))\n",
    "\n",
    "# np. reduce - i tu nawiązanie do znanej techniki przetwarzania rozproszonego - MapReduce\n",
    "# więcej: https://en.wikipedia.org/wiki/MapReduce\n",
    "# oraz: https://wiadrodanych.pl/big-data/jak-dziala-mapreduce/\n",
    "\n",
    "# początkowa akcja jest taka sama (podział i liczba wyrazów), ale w następnej kolejności\n",
    "# wywołujemy reduce, które zsumuje liczbę wyrazów, tutaj funkcją anonimową (lambda)\n",
    "pan_tadeusz_file.map(lambda s: len(s.split())).reduce(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e577a083-a73f-4b4e-aa2e-9ff5abf9be64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69095"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lub tak - ten sam efekt\n",
    "\n",
    "# każdy operator w Pythonie ma swoją dedykowaną metodę - tu wszystkie jest obiektem\n",
    "from operator import add\n",
    "\n",
    "# wywołujemy reduce z użyciem add, które ja spojrzeć w jej sygnaturę przyjmuje dwa argumenty, i zwraca sumę (o ile to możliwe)\n",
    "pan_tadeusz_file.map(lambda s: len(s.split())).reduce(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a175a960-3ac6-4c45-bf51-a8cc6e83ca92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same as a + b.\n"
     ]
    }
   ],
   "source": [
    "# dokumentacja dla operatora add\n",
    "print(add.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "01115c0d-e2b1-4c2a-8812-6dc30f7b7537",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['I', 'zaraz', 'mogłem', 'pieszo,', 'do', 'Twych', 'świątyń', 'progu'],\n",
       " ['Iść', 'za', 'wrócone', 'życie', 'podziękować', 'Bogu),'],\n",
       " ['Tak', 'nas', 'powrócisz', 'cudem', 'na', 'Ojczyzny', 'łono.'],\n",
       " ['Tymczasem', 'przenoś', 'moją', 'duszę', 'utęsknioną'],\n",
       " ['Do', 'tych', 'pagórków', 'leśnych,', 'do', 'tych', 'łąk', 'zielonych,'],\n",
       " ['Szeroko', 'nad', 'błękitnym', 'Niemnem', 'rozciągnionych;'],\n",
       " ['Do', 'tych', 'pól', 'malowanych', 'zbożem', 'rozmaitem,'],\n",
       " ['Wyzłacanych', 'pszenicą,', 'posrebrzanych', 'żytem;'],\n",
       " ['Gdzie', 'bursztynowy', 'świerzop,', 'gryka', 'jak', 'śnieg', 'biała,'],\n",
       " ['Gdzie', 'panieńskim', 'rumieńcem', 'dzięcielina', 'pała,']]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'zaraz',\n",
       " 'mogłem',\n",
       " 'pieszo,',\n",
       " 'do',\n",
       " 'Twych',\n",
       " 'świątyń',\n",
       " 'progu',\n",
       " 'Iść',\n",
       " 'za']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# różnica między map() a flatMap() dla tego przypadku\n",
    "display(pan_tadeusz_file.map(lambda s: s.split()).take(10))\n",
    "pan_tadeusz_file.flatMap(lambda s: s.split()).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113c9de6-9853-447b-8cbb-73bd9c42a50c",
   "metadata": {},
   "source": [
    "### Zadania"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0440a501-ff7a-4d48-a72a-f4d348990bd8",
   "metadata": {},
   "source": [
    "**Zadanie 1**  \n",
    "Wykorzystując ten sam plik z treścią Pana Tadeusza policz i wyświetl:\n",
    "* 1.1 - liczbę linii w tym pliku,\n",
    "* 1.2 - 10 najdłuższych linii - ich długość oraz faktyczne linie - oddzielnie polecenia,\n",
    "* 1.3 - listę wszystkich unikalnych wyrazów w tym pliku,\n",
    "* 1.4 - kolekcję, która zawiera unikalne wyrazy i liczbę ich wystąpień w pliku,\n",
    "* 1.5 - z kolekcji z punktu 1.5, wyświetl 10 najczęściej występujących wyrazów,\n",
    "* 1.6 - tylko te linie, które zawierają więcej niż 7 wyrazów,\n",
    "* 1.7 - wszystkie linie, które zawierają słowo 'Tadeusz',\n",
    "* 1.8 - 10 ostatnich linii, ale zapisanych małymi literami,\n",
    "* 1.9 - sumę znaków w tym pliku.\n",
    "\n",
    "**Zadanie 2**  \n",
    "Wykorzystując listę stopwords z adresu https://github.com/bieli/stopwords/blob/master/polish.stopwords.txt wykorzystaj akcje i transformacje RDD i wygeneruj listę unikalnych słów z pliku z treścią Pana Tadeusza pomijając powyższe słowa stop oraz wszelkie znaki przestankowe. Wynik zapisz do słownika, a następnie do pliku json o nazwie pan_tadeusz_bag_of_words.json.\n",
    "Które słowo występuje w tym tekście najczęściej? Wyświetl je z wyników wygenerowanych powyżej.\n",
    "\n",
    "**Podpowiedzi**\n",
    "\n",
    "W przypadku **zadania 1** większość funkcji, których trzeba użyć znajduje się w API PySpark RDD: https://spark.apache.org/docs/3.5.5/api/python/reference/api/pyspark.RDD.html\n",
    "\n",
    "Dla konkretnych zadań (niektóre pomijam, gdyż są proste) moje sugestie (zapewne można użyć w niektórych przypadkach innych funkcji):\n",
    "* 1.2 - `top()`, `take()`, `map()`, `sortBy()`\n",
    "* 1.3 - `distinct()`\n",
    "* 1.4 - `countByValue()`\n",
    "* 1.5 - wykorzystałem sortowanie słownika z pkt. 1.4 funkcją `sorted()`, której można przekazać klucz, według, którego sortowanie powinno zostać wykonane. Tutaj chodzi o to, aby sortować po wartości z par (klucz, wartość), które znajdują się w słowniku\n",
    "* 1.6, 1.7 - `filter()`\n",
    "* 1.8 - tu nie ma wbudowanej funkcji, która zwraca `n` ostatnich elementów. Można zwrócić wszystkie, a następnie zamienić na małe litery i zwrócić tylko wycinek z ostatnimi 10 elementami.\n",
    "* 1.9 - `map()`, `reduce()`\n",
    "\n",
    "**zadanie 2**\n",
    "\n",
    "Zadanie można zrealizować wykorzystując funkcje przedstawione w podpowiedziach do zadania 1. Dodatkowo:\n",
    "* można napisać własną funkcję do usuwania znaków przestankowych i poprzez `map` oraz `lambdę` ją wywołać na każdym słowie,\n",
    "* zapisanie słownika do pliku json jest dość proste, wykorzystaj moduł `json` oraz metodę `dumps()`,\n",
    "* co do najczęściej występującego słowa - w części 1 zadań było już podobne zadanie."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
